{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Time Ordering\n",
    "\n",
    "**Problem Identified:**\n",
    "- Original log files may not be properly time-ordered\n",
    "- This could affect incident analysis accuracy\n",
    "- Need clean, chronologically sorted dataset\n",
    "\n",
    "**Cleaning Objectives:**\n",
    "1. Parse and validate all log entries\n",
    "2. Sort by timestamp chronologically\n",
    "3. Identify and flag any time gaps or anomalies\n",
    "4. Create clean, ordered output files\n",
    "5. Verify data integrity and continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Environment Ready!\n",
      "Starting comprehensive data validation and ordering...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\" Data Cleaning Environment Ready!\")\n",
    "print(\" Starting comprehensive data validation and ordering...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raw Data Inspection & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSPECTING: time6.txt\n",
      "==================================================\n",
      " FILE STATISTICS:\n",
      "  Total lines: 141,657\n",
      "  Valid timestamps: 141,657\n",
      "  Parse errors: 0\n",
      "\n",
      "‚è∞ TIME ORDERING:\n",
      "  First timestamp: 2025-11-06 00:00:00.850000\n",
      "  Last timestamp: 2025-11-06 23:59:56.860000\n",
      "  Duration: 24.0 hours\n",
      "  Chronologically sorted: ‚ùå NO\n",
      "  Duplicate timestamps: 229\n",
      "  Large time gaps (>5min): 0\n",
      "\n",
      "üîç INSPECTING: time.txt\n",
      "==================================================\n",
      "üìä FILE STATISTICS:\n",
      "  Total lines: 16,529\n",
      "  Valid timestamps: 16,529\n",
      "  Parse errors: 0\n",
      "\n",
      "‚è∞ TIME ORDERING:\n",
      "  First timestamp: 2025-11-07 00:00:00.700000\n",
      "  Last timestamp: 2025-11-07 03:33:49.509000\n",
      "  Duration: 3.6 hours\n",
      "  Chronologically sorted: ‚ùå NO\n",
      "  Duplicate timestamps: 21\n",
      "  Large time gaps (>5min): 0\n"
     ]
    }
   ],
   "source": [
    "def inspect_raw_file(filepath):\n",
    "    \"\"\"\n",
    "    Inspect raw file for time ordering and data quality issues\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç INSPECTING: {filepath}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    timestamps = []\n",
    "    parse_errors = []\n",
    "    line_count = 0\n",
    "    \n",
    "    # Read and extract timestamps\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line_num, line in enumerate(file, 1):\n",
    "            line_count += 1\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Extract timestamp\n",
    "            pattern = r'eta_\\w+\\.\\d+:([\\d/]+) ([\\d:.]+) \\d+ TOOK [\\d.]+s'\n",
    "            match = re.match(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                date_str, time_str = match.groups()\n",
    "                try:\n",
    "                    dt = datetime.strptime(f\"{date_str} {time_str}\", \"%d/%m/%Y %H:%M:%S.%f\")\n",
    "                    timestamps.append({'line': line_num, 'timestamp': dt, 'raw_line': line})\n",
    "                except ValueError as e:\n",
    "                    parse_errors.append({'line': line_num, 'error': str(e), 'raw_line': line[:100]})\n",
    "            else:\n",
    "                parse_errors.append({'line': line_num, 'error': 'Pattern mismatch', 'raw_line': line[:100]})\n",
    "    \n",
    "    print(f\" FILE STATISTICS:\")\n",
    "    print(f\"  Total lines: {line_count:,}\")\n",
    "    print(f\"  Valid timestamps: {len(timestamps):,}\")\n",
    "    print(f\"  Parse errors: {len(parse_errors):,}\")\n",
    "    \n",
    "    if timestamps:\n",
    "        # Convert to DataFrame for analysis\n",
    "        df_temp = pd.DataFrame(timestamps)\n",
    "        \n",
    "        # Check time ordering\n",
    "        is_sorted = df_temp['timestamp'].is_monotonic_increasing\n",
    "        \n",
    "        print(f\"\\n TIME ORDERING:\")\n",
    "        print(f\"  First timestamp: {df_temp['timestamp'].min()}\")\n",
    "        print(f\"  Last timestamp: {df_temp['timestamp'].max()}\")\n",
    "        print(f\"  Duration: {(df_temp['timestamp'].max() - df_temp['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "        print(f\"  Chronologically sorted: {' YES' if is_sorted else ' NO'}\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df_temp.duplicated('timestamp').sum()\n",
    "        print(f\"  Duplicate timestamps: {duplicates:,}\")\n",
    "        \n",
    "        # Check for time gaps\n",
    "        df_temp = df_temp.sort_values('timestamp')\n",
    "        time_diffs = df_temp['timestamp'].diff()\n",
    "        large_gaps = time_diffs[time_diffs > timedelta(minutes=5)]\n",
    "        \n",
    "        print(f\"  Large time gaps (>5min): {len(large_gaps):,}\")\n",
    "        \n",
    "        if len(large_gaps) > 0:\n",
    "            print(f\"    Largest gap: {large_gaps.max()}\")\n",
    "        \n",
    "        # Show ordering issues\n",
    "        if not is_sorted:\n",
    "            # Find where ordering breaks\n",
    "            for i in range(1, min(len(df_temp), 1000)):\n",
    "                if df_temp.iloc[i]['timestamp'] < df_temp.iloc[i-1]['timestamp']:\n",
    "                    print(f\"\\n FIRST ORDERING ISSUE FOUND:\")\n",
    "                    print(f\"    Line {df_temp.iloc[i-1]['line']}: {df_temp.iloc[i-1]['timestamp']}\")\n",
    "                    print(f\"    Line {df_temp.iloc[i]['line']}: {df_temp.iloc[i]['timestamp']}\")\n",
    "                    break\n",
    "    \n",
    "    # Show parse errors (first few)\n",
    "    if parse_errors:\n",
    "        print(f\"\\n PARSE ERRORS (first 5):\")\n",
    "        for error in parse_errors[:5]:\n",
    "            print(f\"    Line {error['line']}: {error['error']}\")\n",
    "            print(f\"      {error['raw_line']}\")\n",
    "    \n",
    "    return timestamps, parse_errors\n",
    "\n",
    "# Inspect both files\n",
    "time6_data, time6_errors = inspect_raw_file('time6.txt')\n",
    "time7_data, time7_errors = inspect_raw_file('time.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean Data Parsing & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CLEANING: time6.txt (time6.txt)\n",
      "  Parsed records: 141,657\n",
      "  Valid records: 141,657\n",
      "  Records with issues: 0\n",
      "  Total validation issues: 0\n",
      "\n",
      " CLEANING: time.txt (time.txt)\n",
      "  Parsed records: 16,529\n",
      "  Valid records: 16,529\n",
      "  Records with issues: 0\n",
      "  Total validation issues: 0\n"
     ]
    }
   ],
   "source": [
    "def parse_clean_logs(filepath, file_label):\n",
    "    \"\"\"\n",
    "    Parse logs with comprehensive validation and cleaning\n",
    "    \"\"\"\n",
    "    print(f\"\\n CLEANING: {filepath} ({file_label})\")\n",
    "    \n",
    "    clean_records = []\n",
    "    validation_issues = []\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        for line_num, line in enumerate(file, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Full pattern matching\n",
    "            pattern = r'(eta_\\w+)\\.(\\d+):([\\d/]+) ([\\d:.]+) (\\d+) TOOK ([\\d.]+)s'\n",
    "            match = re.match(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                agent_type, pid, date_str, time_str, transaction_id, execution_time = match.groups()\n",
    "                \n",
    "                try:\n",
    "                    # Parse and validate all components\n",
    "                    dt = datetime.strptime(f\"{date_str} {time_str}\", \"%d/%m/%Y %H:%M:%S.%f\")\n",
    "                    pid_int = int(pid)\n",
    "                    txn_id = int(transaction_id)\n",
    "                    exec_time = float(execution_time)\n",
    "                    \n",
    "                    # Validation checks\n",
    "                    issues = []\n",
    "                    \n",
    "                    # Check for reasonable values\n",
    "                    if exec_time < 0:\n",
    "                        issues.append(\"Negative execution time\")\n",
    "                    elif exec_time > 300:  # >5 minutes seems suspicious\n",
    "                        issues.append(f\"Extremely long execution time: {exec_time}s\")\n",
    "                    \n",
    "                    if pid_int <= 0:\n",
    "                        issues.append(\"Invalid PID\")\n",
    "                    \n",
    "                    if txn_id <= 0:\n",
    "                        issues.append(\"Invalid transaction ID\")\n",
    "                    \n",
    "                    # Check date reasonableness (should be Nov 6-7, 2025)\n",
    "                    if not (datetime(2025, 11, 6) <= dt <= datetime(2025, 11, 8)):\n",
    "                        issues.append(f\"Date outside expected range: {dt.date()}\")\n",
    "                    \n",
    "                    # Store record with metadata\n",
    "                    record = {\n",
    "                        'source_file': file_label,\n",
    "                        'line_number': line_num,\n",
    "                        'agent_type': agent_type,\n",
    "                        'pid': pid_int,\n",
    "                        'datetime': dt,\n",
    "                        'transaction_id': txn_id,\n",
    "                        'execution_time': exec_time,\n",
    "                        'raw_line': line,\n",
    "                        'validation_issues': issues,\n",
    "                        'is_valid': len(issues) == 0\n",
    "                    }\n",
    "                    \n",
    "                    clean_records.append(record)\n",
    "                    \n",
    "                    if issues:\n",
    "                        validation_issues.extend([{\n",
    "                            'line': line_num, \n",
    "                            'issue': issue, \n",
    "                            'record': record\n",
    "                        } for issue in issues])\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    validation_issues.append({\n",
    "                        'line': line_num,\n",
    "                        'issue': f\"Parse error: {str(e)}\",\n",
    "                        'raw_line': line\n",
    "                    })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(clean_records)\n",
    "    \n",
    "    print(f\"  Parsed records: {len(clean_records):,}\")\n",
    "    print(f\"  Valid records: {df['is_valid'].sum():,}\")\n",
    "    print(f\"  Records with issues: {(~df['is_valid']).sum():,}\")\n",
    "    print(f\"  Total validation issues: {len(validation_issues):,}\")\n",
    "    \n",
    "    return df, validation_issues\n",
    "\n",
    "# Parse both files with cleaning\n",
    "df_time6_clean, time6_issues = parse_clean_logs('time6.txt', 'time6.txt')\n",
    "df_time7_clean, time7_issues = parse_clean_logs('time.txt', 'time.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine and Sort Chronologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " COMBINING DATASETS\n",
      "==============================\n",
      "Combined dataset size: 158,186 records\n",
      "Time6.txt: 141,657 records\n",
      "Time.txt: 16,529 records\n",
      "\n",
      " SORTING CHRONOLOGICALLY...\n",
      " Chronological sorting completed\n",
      " First record: 2025-11-06 00:00:00.850000\n",
      " Last record: 2025-11-07 03:33:49.509000\n",
      " Total duration: 27.6 hours\n",
      "\n",
      " FILE BOUNDARY ANALYSIS:\n",
      " Time6.txt last record: 2025-11-06 23:59:56.860000\n",
      " Time.txt first record: 2025-11-07 00:00:00.700000\n",
      " Time gap between files: 0.1 minutes\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets\n",
    "print(\"\\n COMBINING DATASETS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Combine all records\n",
    "df_combined = pd.concat([df_time6_clean, df_time7_clean], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset size: {len(df_combined):,} records\")\n",
    "print(f\"Time6.txt: {len(df_time6_clean):,} records\")\n",
    "print(f\"Time.txt: {len(df_time7_clean):,} records\")\n",
    "\n",
    "# Sort chronologically\n",
    "print(f\"\\n SORTING CHRONOLOGICALLY...\")\n",
    "df_sorted = df_combined.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Verify sorting\n",
    "print(f\" Chronological sorting completed\")\n",
    "print(f\" First record: {df_sorted['datetime'].iloc[0]}\")\n",
    "print(f\" Last record: {df_sorted['datetime'].iloc[-1]}\")\n",
    "print(f\" Total duration: {(df_sorted['datetime'].iloc[-1] - df_sorted['datetime'].iloc[0]).total_seconds() / 3600:.1f} hours\")\n",
    "\n",
    "# Check for overlaps between files\n",
    "if len(df_time6_clean) > 0 and len(df_time7_clean) > 0:\n",
    "    time6_max = df_time6_clean['datetime'].max()\n",
    "    time7_min = df_time7_clean['datetime'].min()\n",
    "    \n",
    "    print(f\"\\n FILE BOUNDARY ANALYSIS:\")\n",
    "    print(f\"  Time6.txt last record: {time6_max}\")\n",
    "    print(f\"  Time.txt first record: {time7_min}\")\n",
    "    \n",
    "    if time6_max > time7_min:\n",
    "        overlap_duration = (time6_max - time7_min).total_seconds() / 60\n",
    "        print(f\" Time overlap detected: {overlap_duration:.1f} minutes\")\n",
    "        \n",
    "        # Check for duplicate transactions in overlap\n",
    "        overlap_records = df_sorted[\n",
    "            (df_sorted['datetime'] >= time7_min) &\n",
    "            (df_sorted['datetime'] <= time6_max)\n",
    "        ]\n",
    "        overlap_from_time6 = overlap_records[overlap_records['source_file'] == 'time6.txt']\n",
    "        overlap_from_time7 = overlap_records[overlap_records['source_file'] == 'time.txt']\n",
    "        \n",
    "        print(f\"    Records from time6.txt in overlap: {len(overlap_from_time6)}\")\n",
    "        print(f\"    Records from time.txt in overlap: {len(overlap_from_time7)}\")\n",
    "        \n",
    "        # Check for exact duplicates\n",
    "        duplicates = df_sorted.duplicated(['datetime', 'pid', 'transaction_id'], keep=False)\n",
    "        if duplicates.sum() > 0:\n",
    "            print(f\" Duplicate transactions found: {duplicates.sum()}\")\n",
    "        else:\n",
    "            print(f\" No duplicate transactions found\")\n",
    "    else:\n",
    "        gap_duration = (time7_min - time6_max).total_seconds() / 60\n",
    "        print(f\" Time gap between files: {gap_duration:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DATA QUALITY ASSESSMENT\n",
      "========================================\n",
      " DATASET OVERVIEW:\n",
      "  Total records: 158,186\n",
      "  Valid records: 158,186 (100.0%)\n",
      "  Invalid records: 0 (0.0%)\n",
      "\n",
      " TIME CONTINUITY:\n",
      " Average time between records: 0.6 seconds\n",
      " Median time between records: 0.4 seconds\n",
      " Maximum gap: 0.4 minutes\n",
      " No significant gaps detected\n",
      "\n",
      " PERFORMANCE DATA QUALITY:\n",
      "  Min execution time: 0.019s\n",
      "  Max execution time: 119.2s\n",
      "  Mean execution time: 3.692s\n",
      "  Median execution time: 1.648s\n",
      "  Extremely fast (<0.01s): 0\n",
      "  Extremely slow (>2min): 0\n",
      "\n",
      " SYSTEM COMPONENTS:\n",
      "  Unique agent types: 2\n",
      "  Agent type distribution: {'eta_agent': 148122, 'eta_iagent': 10064}\n",
      "  Unique PIDs: 70\n",
      "  PID range: 7690 - 8216\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Overall statistics\n",
    "valid_records = df_sorted[df_sorted['is_valid']]\n",
    "invalid_records = df_sorted[~df_sorted['is_valid']]\n",
    "\n",
    "print(f\" DATASET OVERVIEW:\")\n",
    "print(f\"  Total records: {len(df_sorted):,}\")\n",
    "print(f\"  Valid records: {len(valid_records):,} ({len(valid_records)/len(df_sorted)*100:.1f}%)\")\n",
    "print(f\"  Invalid records: {len(invalid_records):,} ({len(invalid_records)/len(df_sorted)*100:.1f}%)\")\n",
    "\n",
    "# Time continuity analysis\n",
    "print(f\"\\n TIME CONTINUITY:\")\n",
    "if len(valid_records) > 1:\n",
    "    time_diffs = valid_records['datetime'].diff().dropna()\n",
    "    \n",
    "    print(f\"  Average time between records: {time_diffs.mean().total_seconds():.1f} seconds\")\n",
    "    print(f\"  Median time between records: {time_diffs.median().total_seconds():.1f} seconds\")\n",
    "    print(f\"  Maximum gap: {time_diffs.max().total_seconds() / 60:.1f} minutes\")\n",
    "    \n",
    "    # Identify significant gaps\n",
    "    large_gaps = time_diffs[time_diffs > timedelta(minutes=5)]\n",
    "    if len(large_gaps) > 0:\n",
    "        print(f\"   Large gaps (>5min): {len(large_gaps)}\")\n",
    "        print(f\"   Largest gap: {large_gaps.max().total_seconds() / 60:.1f} minutes\")\n",
    "    else:\n",
    "        print(f\"   No significant gaps detected\")\n",
    "\n",
    "# Performance data quality\n",
    "if len(valid_records) > 0:\n",
    "    exec_times = valid_records['execution_time']\n",
    "    \n",
    "    print(f\"\\n PERFORMANCE DATA QUALITY:\")\n",
    "    print(f\"  Min execution time: {exec_times.min():.3f}s\")\n",
    "    print(f\"  Max execution time: {exec_times.max():.1f}s\")\n",
    "    print(f\"  Mean execution time: {exec_times.mean():.3f}s\")\n",
    "    print(f\"  Median execution time: {exec_times.median():.3f}s\")\n",
    "    \n",
    "    # Check for anomalies\n",
    "    extremely_fast = (exec_times < 0.01).sum()\n",
    "    extremely_slow = (exec_times > 120).sum()\n",
    "    \n",
    "    print(f\"  Extremely fast (<0.01s): {extremely_fast:,}\")\n",
    "    print(f\"  Extremely slow (>2min): {extremely_slow:,}\")\n",
    "\n",
    "# Agent and PID analysis\n",
    "if len(valid_records) > 0:\n",
    "    print(f\"\\n SYSTEM COMPONENTS:\")\n",
    "    print(f\"  Unique agent types: {valid_records['agent_type'].nunique()}\")\n",
    "    print(f\"  Agent type distribution: {valid_records['agent_type'].value_counts().to_dict()}\")\n",
    "    print(f\"  Unique PIDs: {valid_records['pid'].nunique()}\")\n",
    "    print(f\"  PID range: {valid_records['pid'].min()} - {valid_records['pid'].max()}\")\n",
    "\n",
    "# Show validation issues summary\n",
    "if time6_issues or time7_issues:\n",
    "    all_issues = time6_issues + time7_issues\n",
    "    print(f\"\\n VALIDATION ISSUES SUMMARY:\")\n",
    "    \n",
    "    # Group issues by type\n",
    "    issue_types = {}\n",
    "    for issue in all_issues:\n",
    "        issue_type = issue['issue']\n",
    "        if issue_type not in issue_types:\n",
    "            issue_types[issue_type] = 0\n",
    "        issue_types[issue_type] += 1\n",
    "    \n",
    "    for issue_type, count in sorted(issue_types.items()):\n",
    "        print(f\"    {issue_type}: {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EXPORTING CLEAN DATASET\n",
      "===================================\n",
      " Clean dataset exported: cleaned_eta_logs.csv\n",
      "  Records: 158,186\n",
      "  Columns: 13\n",
      "  File size: 16.7 MB\n",
      "\n",
      " FINAL DATASET SUMMARY:\n",
      "  total_raw_records: 158,186\n",
      "  valid_records: 158,186\n",
      "  invalid_records: 0\n",
      "  time_range:\n",
      "    start: 2025-11-06 00:00:00.850000\n",
      "    end: 2025-11-07 03:33:49.509000\n",
      "    duration_hours: 27.56351638888889\n",
      "  data_sources:\n",
      "    time6_records: 141657\n",
      "    time7_records: 16529\n",
      "\n",
      " DATA CLEANING COMPLETE!\n",
      "   Clean dataset ready for incident analysis\n",
      "   Use 'cleaned_eta_logs.csv' for all subsequent analysis\n",
      "   Data quality: 100.0% valid records\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n EXPORTING CLEAN DATASET\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create clean subset (only valid records)\n",
    "df_clean_final = valid_records[[\n",
    "    'datetime', 'agent_type', 'pid', 'transaction_id', 'execution_time', \n",
    "    'source_file', 'line_number'\n",
    "]].copy()\n",
    "\n",
    "# Add derived fields for analysis\n",
    "df_clean_final['date'] = df_clean_final['datetime'].dt.date\n",
    "df_clean_final['hour'] = df_clean_final['datetime'].dt.hour\n",
    "df_clean_final['minute'] = df_clean_final['datetime'].dt.minute\n",
    "df_clean_final['is_slow'] = df_clean_final['execution_time'] > 20\n",
    "df_clean_final['is_very_slow'] = df_clean_final['execution_time'] > 30\n",
    "df_clean_final['is_critical'] = df_clean_final['execution_time'] > 60\n",
    "\n",
    "# Export to CSV for further analysis\n",
    "clean_filename = 'cleaned_eta_logs.csv'\n",
    "df_clean_final.to_csv(clean_filename, index=False)\n",
    "\n",
    "print(f\" Clean dataset exported: {clean_filename}\")\n",
    "print(f\"  Records: {len(df_clean_final):,}\")\n",
    "print(f\"  Columns: {len(df_clean_final.columns)}\")\n",
    "print(f\"  File size: {os.path.getsize(clean_filename) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Export validation report\n",
    "validation_report = {\n",
    "    'total_raw_records': len(df_combined),\n",
    "    'valid_records': len(valid_records),\n",
    "    'invalid_records': len(invalid_records),\n",
    "    'time_range': {\n",
    "        'start': str(df_clean_final['datetime'].min()),\n",
    "        'end': str(df_clean_final['datetime'].max()),\n",
    "        'duration_hours': (df_clean_final['datetime'].max() - df_clean_final['datetime'].min()).total_seconds() / 3600\n",
    "    },\n",
    "    'data_sources': {\n",
    "        'time6_records': len(df_time6_clean),\n",
    "        'time7_records': len(df_time7_clean)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n FINAL DATASET SUMMARY:\")\n",
    "for key, value in validation_report.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"    {subkey}: {subvalue}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n DATA CLEANING COMPLETE\")\n",
    "print(f\"   Clean dataset ready for incident analysis\")\n",
    "print(f\"   Use '{clean_filename}' for all subsequent analysis\")\n",
    "print(f\"   Data quality: {len(valid_records)/len(df_combined)*100:.1f}% valid records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Incident Timeline Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " INCIDENT TIMELINE VERIFICATION WITH CLEAN DATA\n",
      "=======================================================\n",
      " ANALYSIS WINDOW DATA:\n",
      "  Records in analysis window: 30,818\n",
      "  Time range: 2025-11-06 21:30:04.074000 ‚Üí 2025-11-07 03:29:59.972000\n",
      "\n",
      " HOURLY BREAKDOWN (clean data):\n",
      "  21:00: 2756.0 txns,   2.0s avg,  0.4% slow\n",
      "  22:00: 5896.0 txns,  12.1s avg, 15.1% slow\n",
      "  23:00: 5887.0 txns,  10.8s avg, 10.8% slow\n",
      "  00:00: 5777.0 txns,  10.5s avg,  9.9% slow\n",
      "  01:00: 4644.0 txns,   5.0s avg,  3.4% slow\n",
      "  02:00: 3981.0 txns,   1.8s avg,  0.2% slow\n",
      "  03:00: 1877.0 txns,   1.1s avg,  0.0% slow\n",
      "\n",
      " CLEAN DATA INSIGHTS:\n",
      "  Worst performance: 22:00 (12.1s avg)\n",
      "  Highest slow rate: 22:00 (15.1%)\n",
      "  Critical transactions (>60s): 179\n",
      "  Worst transaction: 115.9s at 22:25:27\n",
      "\n",
      " Clean data ready for accurate incident analysis!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/nix-shell.aQbSAD/ipykernel_63401/1455007149.py:21: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly = analysis_data.groupby(analysis_data['datetime'].dt.floor('H')).agg({\n"
     ]
    }
   ],
   "source": [
    "# Quick verification of incident timeframe with clean data\n",
    "print(\"\\n INCIDENT TIMELINE VERIFICATION WITH CLEAN DATA\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "incident_time = datetime(2025, 11, 6, 22, 10)\n",
    "analysis_start = datetime(2025, 11, 6, 21, 30)\n",
    "analysis_end = datetime(2025, 11, 7, 3, 30)\n",
    "\n",
    "# Filter to analysis window\n",
    "analysis_data = df_clean_final[\n",
    "    (df_clean_final['datetime'] >= analysis_start) &\n",
    "    (df_clean_final['datetime'] <= analysis_end)\n",
    "].copy()\n",
    "\n",
    "print(f\" ANALYSIS WINDOW DATA:\")\n",
    "print(f\"  Records in analysis window: {len(analysis_data):,}\")\n",
    "print(f\"  Time range: {analysis_data['datetime'].min()} ‚Üí {analysis_data['datetime'].max()}\")\n",
    "\n",
    "if len(analysis_data) > 0:\n",
    "    # Hour-by-hour breakdown\n",
    "    hourly = analysis_data.groupby(analysis_data['datetime'].dt.floor('H')).agg({\n",
    "        'execution_time': ['count', 'mean'],\n",
    "        'is_slow': 'sum',\n",
    "        'is_critical': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    hourly.columns = ['count', 'avg_time', 'slow_count', 'critical_count']\n",
    "    hourly['slow_pct'] = (hourly['slow_count'] / hourly['count'] * 100).round(1)\n",
    "    \n",
    "    print(f\"\\n HOURLY BREAKDOWN (clean data):\")\n",
    "    for hour, row in hourly.iterrows():\n",
    "        print(f\"  {hour.strftime('%H:00')}: {row['count']:>4} txns, {row['avg_time']:>5.1f}s avg, {row['slow_pct']:>4.1f}% slow\")\n",
    "    \n",
    "    # Identify peak issues\n",
    "    worst_hour = hourly['avg_time'].idxmax()\n",
    "    highest_slow_rate = hourly['slow_pct'].idxmax()\n",
    "    \n",
    "    print(f\"\\n CLEAN DATA INSIGHTS:\")\n",
    "    print(f\"  Worst performance: {worst_hour.strftime('%H:00')} ({hourly.loc[worst_hour, 'avg_time']:.1f}s avg)\")\n",
    "    print(f\"  Highest slow rate: {highest_slow_rate.strftime('%H:00')} ({hourly.loc[highest_slow_rate, 'slow_pct']:.1f}%)\")\n",
    "    \n",
    "    # Critical transactions\n",
    "    critical_txns = analysis_data[analysis_data['is_critical']]\n",
    "    if len(critical_txns) > 0:\n",
    "        print(f\"  Critical transactions (>60s): {len(critical_txns):,}\")\n",
    "        worst_txn = critical_txns.loc[critical_txns['execution_time'].idxmax()]\n",
    "        print(f\"  Worst transaction: {worst_txn['execution_time']:.1f}s at {worst_txn['datetime'].strftime('%H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n Clean data ready for accurate incident analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
