{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98caec67",
   "metadata": {},
   "source": [
    "\n",
    "# ETA Performance — Discovery Notebook (Unsupervised Baseline & Peak)\n",
    "\n",
    "**Generated:** 2025-11-28 19:57:26\n",
    "\n",
    "This notebook **automatically discovers** baseline and peak (incident) windows from `cleaned_eta_logs.csv` using\n",
    "per-minute aggregation and robust anomaly detection on the **p95 execution time**. It then verifies degradation and\n",
    "**compares** the discovered windows against any previously saved manual windows (if `artifact_verification_case.json` is present).\n",
    "\n",
    "**Data file SHA256:** ``\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3922d",
   "metadata": {},
   "source": [
    "\n",
    "## Environment & Requirements\n",
    "- Python 3.9+\n",
    "- `pandas`, `numpy`, `matplotlib`\n",
    "\n",
    "> No internet required. All plots are generated locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d30f5744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: datasets/cleaned_eta_logs.csv\n",
      "SHA256 : b7c4fe3646d472cf92b9221abd54302fb82712f5969e9dabb942b2f459dabacd\n",
      "Python : 3.12.11\n",
      "Pandas : 2.2.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib, platform, json, os\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.options.display.max_rows = 50\n",
    "\n",
    "CSV_PATH = 'datasets/cleaned_eta_logs.csv'\n",
    "\n",
    "# Dataset fingerprint for audit trail\n",
    "try:\n",
    "    sha = hashlib.sha256(open(CSV_PATH,'rb').read()).hexdigest()\n",
    "except FileNotFoundError:\n",
    "    sha = None\n",
    "print('Dataset:', CSV_PATH)\n",
    "print('SHA256 :', sha)\n",
    "print('Python :', platform.python_version())\n",
    "print('Pandas :', pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d37daf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows   : 158186\n",
      "Start  : 2025-11-06 00:00:00.850000\n",
      "End    : 2025-11-07 03:33:49.509000\n",
      "Agents : 2 ['eta_agent', 'eta_iagent']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Load & normalize\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert 'datetime' in df.columns and 'execution_time' in df.columns, 'CSV must contain datetime and execution_time columns'\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "for col in ['is_slow','is_very_slow','is_critical']:\n",
    "    if col in df.columns and df[col].dtype != bool:\n",
    "        df[col] = df[col].astype(str).str.lower().map({'true':True,'false':False})\n",
    "\n",
    "print('Rows   :', len(df))\n",
    "print('Start  :', df['datetime'].min())\n",
    "print('End    :', df['datetime'].max())\n",
    "print('Agents :', df['agent_type'].nunique(), sorted(df['agent_type'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4803f1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes aggregated: 1654\n",
      "Median p95: 5.775 MAD: 2.937\n",
      "Anomaly minutes (z>3): 202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Per-minute aggregation & robust anomaly detection\n",
    "\n",
    "df_min = df.copy()\n",
    "df_min['minute_ts'] = df_min['datetime'].dt.floor('min')\n",
    "agg = df_min.groupby('minute_ts').agg(\n",
    "    n=('execution_time','size'),\n",
    "    mean_exec=('execution_time','mean'),\n",
    "    p95_exec=('execution_time', lambda s: s.quantile(0.95)),\n",
    "    max_exec=('execution_time','max')\n",
    ").sort_index()\n",
    "\n",
    "# Robust z on per-minute p95\n",
    "med = agg['p95_exec'].median()\n",
    "mad = (agg['p95_exec'] - med).abs().median()\n",
    "agg['robust_z'] = 0 if mad==0 else 0.6745 * (agg['p95_exec'] - med) / mad\n",
    "\n",
    "print('Minutes aggregated:', len(agg))\n",
    "print('Median p95:', round(med,3), 'MAD:', round(mad,3))\n",
    "\n",
    "# Heuristic: anomaly minutes where robust_z>3\n",
    "anom = agg[agg['robust_z']>3]\n",
    "print('Anomaly minutes (z>3):', len(anom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c243c036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected PEAK cluster: {'start': Timestamp('2025-11-07 01:26:00'), 'end': Timestamp('2025-11-07 01:27:00'), 'duration_min': 1, 'avg_p95': 50.16209999999997, 'max_p95': 50.16209999999997}\n",
      "Selected BASELINE window: None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSelected BASELINE window:\u001b[39m\u001b[33m'\u001b[39m, best_baseline)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Materialize slices from df\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m baseline_slice = df[(df[\u001b[33m'\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m'\u001b[39m]>=\u001b[43mbest_baseline\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstart\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m) & (df[\u001b[33m'\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m'\u001b[39m]<best_baseline[\u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m])]\n\u001b[32m     99\u001b[39m peak_slice     = df[(df[\u001b[33m'\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m'\u001b[39m]>=peak_window[\u001b[33m'\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m'\u001b[39m]) & (df[\u001b[33m'\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m'\u001b[39m]<peak_window[\u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m])]\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mBaseline count:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(baseline_slice), \u001b[33m'\u001b[39m\u001b[33mPeak count:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(peak_slice))\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Discover peak cluster (incident window) and baseline window automatically\n",
    "# Strategy:\n",
    "# - Group contiguous anomaly minutes into clusters; pick the cluster with highest avg p95_exec.\n",
    "# - Peak window = cluster start -> cluster end.\n",
    "# - Baseline window = same duration window with lowest avg p95_exec that does not overlap anomalies.\n",
    "\n",
    "# Helper to form contiguous clusters\n",
    "clusters = []\n",
    "current = []\n",
    "prev_ts = None\n",
    "for ts in anom.index:\n",
    "    if prev_ts is None or (ts - prev_ts == pd.Timedelta(minutes=1)):\n",
    "        current.append(ts)\n",
    "    else:\n",
    "        clusters.append(current)\n",
    "        current = [ts]\n",
    "    prev_ts = ts\n",
    "if current:\n",
    "    clusters.append(current)\n",
    "\n",
    "cluster_stats = []\n",
    "for cl in clusters:\n",
    "    start = cl[0]; end = cl[-1] + pd.Timedelta(minutes=1)  # exclusive end\n",
    "    block = agg.loc[start:end - pd.Timedelta(seconds=1)]\n",
    "    cluster_stats.append({\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'duration_min': len(cl),\n",
    "        'avg_p95': float(block['p95_exec'].mean()),\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    })\n",
    "\n",
    "# Choose peak: cluster with highest avg_p95 (or max_p95 as tiebreaker)\n",
    "peak_window = None\n",
    "if cluster_stats:\n",
    "    cluster_stats.sort(key=lambda d: (d['avg_p95'], d['max_p95']), reverse=True)\n",
    "    peak_window = cluster_stats[0]\n",
    "    print('Selected PEAK cluster:', peak_window)\n",
    "else:\n",
    "    # Fallback: take top-N minute window by p95_exec average (e.g., 50 minutes)\n",
    "    dur = 50\n",
    "    roll = agg['p95_exec'].rolling(f'{dur}T').mean()\n",
    "    idx = roll.idxmax()\n",
    "    if pd.isna(idx):\n",
    "        raise RuntimeError('No data to form a peak window.')\n",
    "    start = idx - pd.Timedelta(minutes=dur-1)\n",
    "    end = idx + pd.Timedelta(minutes=1)\n",
    "    block = agg.loc[start:end - pd.Timedelta(seconds=1)]\n",
    "    peak_window = {\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'duration_min': len(block),\n",
    "        'avg_p95': float(block['p95_exec'].mean()),\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    }\n",
    "    print('Fallback PEAK window:', peak_window)\n",
    "\n",
    "# Build candidate baseline windows of same duration\n",
    "dur = peak_window['duration_min']\n",
    "start_all = agg.index.min(); end_all = agg.index.max() + pd.Timedelta(minutes=1)\n",
    "\n",
    "# Slide a window of length 'dur' across the timeline to find minimal avg p95, excluding anomaly minutes\n",
    "anom_minutes = set(anom.index)\n",
    "\n",
    "best_baseline = None\n",
    "cursor = start_all\n",
    "while cursor + pd.Timedelta(minutes=dur) <= end_all:\n",
    "    w_start = cursor\n",
    "    w_end = cursor + pd.Timedelta(minutes=dur)\n",
    "    # minutes = pd.date_range(w_start, w_end - pd.Timedelta(minutes=1), freq='T')\n",
    "    minutes = pd.date_range(w_start, w_end - pd.Timedelta(minutes=1), freq='min')\n",
    "    # Exclude if any minute is anomalous or overlaps peak window\n",
    "    if any(m in anom_minutes for m in minutes):\n",
    "        cursor += pd.Timedelta(minutes=1)\n",
    "        continue\n",
    "    if not (w_end <= peak_window['start'] or w_start >= peak_window['end']):\n",
    "        cursor += pd.Timedelta(minutes=1)\n",
    "        continue\n",
    "    block = agg.loc[w_start:w_end - pd.Timedelta(seconds=1)]\n",
    "    if len(block) < max(5, dur*0.8):  # require sufficient coverage\n",
    "        cursor += pd.Timedelta(minutes=1)\n",
    "        continue\n",
    "    avg_p95 = float(block['p95_exec'].mean())\n",
    "    candidate = {\n",
    "        'start': w_start,\n",
    "        'end': w_end,\n",
    "        'duration_min': len(block),\n",
    "        'avg_p95': avg_p95,\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    }\n",
    "    if (best_baseline is None) or (avg_p95 < best_baseline['avg_p95']):\n",
    "        best_baseline = candidate\n",
    "    cursor += pd.Timedelta(minutes=1)\n",
    "\n",
    "print('Selected BASELINE window:', best_baseline)\n",
    "\n",
    "# Materialize slices from df\n",
    "baseline_slice = df[(df['datetime']>=best_baseline['start']) & (df['datetime']<best_baseline['end'])]\n",
    "peak_slice     = df[(df['datetime']>=peak_window['start']) & (df['datetime']<peak_window['end'])]\n",
    "\n",
    "print('Baseline count:', len(baseline_slice), 'Peak count:', len(peak_slice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eca4a030-0825-4a2d-a9db-1d603613e55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected PEAK cluster: {'start': Timestamp('2025-11-07 01:26:00'), 'end': Timestamp('2025-11-07 01:27:00'), 'duration_min': 1, 'avg_p95': 50.16209999999997, 'max_p95': 50.16209999999997}\n",
      "No clean baseline found. Using fallback: earliest window of same duration.\n",
      "Selected BASELINE window: {'start': Timestamp('2025-11-06 00:00:00'), 'end': Timestamp('2025-11-06 00:01:00'), 'duration_min': 1, 'avg_p95': 8.488249999999999, 'max_p95': 8.488249999999999}\n",
      "Baseline count: 66 Peak count: 78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Discover peak cluster (incident window) and baseline window automatically\n",
    "# Strategy:\n",
    "# - Group contiguous anomaly minutes into clusters; pick the cluster with highest avg p95_exec.\n",
    "# - Peak window = cluster start -> cluster end.\n",
    "# - Baseline window = same duration window with lowest avg p95_exec that does not overlap anomalies.\n",
    "\n",
    "# Helper to form contiguous clusters\n",
    "clusters = []\n",
    "current = []\n",
    "prev_ts = None\n",
    "for ts in anom.index:\n",
    "    if prev_ts is None or (ts - prev_ts == pd.Timedelta(minutes=1)):\n",
    "        current.append(ts)\n",
    "    else:\n",
    "        clusters.append(current)\n",
    "        current = [ts]\n",
    "    prev_ts = ts\n",
    "if current:\n",
    "    clusters.append(current)\n",
    "\n",
    "cluster_stats = []\n",
    "for cl in clusters:\n",
    "    start = cl[0]; end = cl[-1] + pd.Timedelta(minutes=1)  # exclusive end\n",
    "    block = agg.loc[start:end - pd.Timedelta(seconds=1)]\n",
    "    cluster_stats.append({\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'duration_min': len(cl),\n",
    "        'avg_p95': float(block['p95_exec'].mean()),\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    })\n",
    "\n",
    "# Choose peak: cluster with highest avg_p95 (or max_p95 as tiebreaker)\n",
    "peak_window = None\n",
    "if cluster_stats:\n",
    "    cluster_stats.sort(key=lambda d: (d['avg_p95'], d['max_p95']), reverse=True)\n",
    "    peak_window = cluster_stats[0]\n",
    "    print('Selected PEAK cluster:', peak_window)\n",
    "else:\n",
    "    # Fallback: take top-N minute window by p95_exec average (e.g., 50 minutes)\n",
    "    dur = 50\n",
    "    roll = agg['p95_exec'].rolling(f'{dur}T').mean()\n",
    "    idx = roll.idxmax()\n",
    "    if pd.isna(idx):\n",
    "        raise RuntimeError('No data to form a peak window.')\n",
    "    start = idx - pd.Timedelta(minutes=dur-1)\n",
    "    end = idx + pd.Timedelta(minutes=1)\n",
    "    block = agg.loc[start:end - pd.Timedelta(seconds=1)]\n",
    "    peak_window = {\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'duration_min': len(block),\n",
    "        'avg_p95': float(block['p95_exec'].mean()),\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    }\n",
    "    print('Fallback PEAK window:', peak_window)\n",
    "\n",
    "# Build candidate baseline windows of same duration\n",
    "dur = peak_window['duration_min']\n",
    "start_all = agg.index.min(); end_all = agg.index.max() + pd.Timedelta(minutes=1)\n",
    "\n",
    "# Slide a window of length 'dur' across the timeline to find minimal avg p95, excluding anomaly minutes\n",
    "anom_minutes = set(anom.index)\n",
    "\n",
    "\n",
    "# If no baseline found, fallback to earliest window of same duration\n",
    "if best_baseline is None:\n",
    "    print(\"No clean baseline found. Using fallback: earliest window of same duration.\")\n",
    "    w_start = agg.index.min()\n",
    "    w_end = w_start + pd.Timedelta(minutes=dur)\n",
    "    block = agg.loc[w_start:w_end - pd.Timedelta(seconds=1)]\n",
    "    best_baseline = {\n",
    "        'start': w_start,\n",
    "        'end': w_end,\n",
    "        'duration_min': len(block),\n",
    "        'avg_p95': float(block['p95_exec'].mean()),\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    }\n",
    "\n",
    "print('Selected BASELINE window:', best_baseline)\n",
    "\n",
    "# Materialize slices\n",
    "baseline_slice = df[(df['datetime'] >= best_baseline['start']) & (df['datetime'] < best_baseline['end'])]\n",
    "peak_slice     = df[(df['datetime'] >= peak_window['start']) & (df['datetime'] < peak_window['end'])]\n",
    "\n",
    "print('Baseline count:', len(baseline_slice), 'Peak count:', len(peak_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a8ff9b7-5016-4ba4-bbeb-7512f1784e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected PEAK cluster: {'start': Timestamp('2025-11-06 22:53:00'), 'end': Timestamp('2025-11-06 22:58:00'), 'duration_min': 5, 'avg_p95': 43.47478999999998, 'max_p95': 74.429}\n",
      "Selected BASELINE window: {'start': Timestamp('2025-11-06 03:23:00'), 'end': Timestamp('2025-11-06 03:28:00'), 'duration_min': 5, 'avg_p95': 1.9064699999999994, 'max_p95': 2.3118999999999987}\n",
      "Baseline count: 272 Peak count: 493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Discover peak cluster (incident window) and baseline window automatically (robust version)\n",
    "\n",
    "MIN_CLUSTER_MINUTES = 5        # ignore clusters shorter than this\n",
    "ROLLING_FALLBACK_MIN = 50      # minutes for rolling fallback window\n",
    "MIN_COUNT_PER_MIN = 30         # throughput guard in candidate windows\n",
    "\n",
    "# Helper: form contiguous anomaly clusters (by minute)\n",
    "clusters, current, prev_ts = [], [], None\n",
    "for ts in anom.index:\n",
    "    if prev_ts is None or (ts - prev_ts == pd.Timedelta(minutes=1)):\n",
    "        current.append(ts)\n",
    "    else:\n",
    "        clusters.append(current)\n",
    "        current = [ts]\n",
    "    prev_ts = ts\n",
    "if current:\n",
    "    clusters.append(current)\n",
    "\n",
    "# Filter clusters by minimum duration\n",
    "clusters = [cl for cl in clusters if len(cl) >= MIN_CLUSTER_MINUTES]\n",
    "\n",
    "cluster_stats = []\n",
    "for cl in clusters:\n",
    "    start = cl[0]; end = cl[-1] + pd.Timedelta(minutes=1)  # exclusive end\n",
    "    block = agg.loc[start:end - pd.Timedelta(seconds=1)]\n",
    "    # Throughput guard: average count/min must be sufficient\n",
    "    if block['n'].mean() < MIN_COUNT_PER_MIN:\n",
    "        continue\n",
    "    cluster_stats.append({\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'duration_min': len(cl),\n",
    "        'avg_p95': float(block['p95_exec'].mean()),\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    })\n",
    "\n",
    "# Choose peak: best cluster by avg_p95 (tiebreaker: max_p95)\n",
    "peak_window = None\n",
    "if cluster_stats:\n",
    "    cluster_stats.sort(key=lambda d: (d['avg_p95'], d['max_p95']), reverse=True)\n",
    "    peak_window = cluster_stats[0]\n",
    "    print('Selected PEAK cluster:', peak_window)\n",
    "else:\n",
    "    # Fallback: use rolling window (e.g., 50 minutes) to find highest-average p95 period\n",
    "    dur = ROLLING_FALLBACK_MIN\n",
    "    roll = agg['p95_exec'].rolling(f'{dur}min').mean()\n",
    "    idx = roll.idxmax()\n",
    "    if pd.isna(idx):\n",
    "        raise RuntimeError('No data to form a peak window.')\n",
    "    start = idx - pd.Timedelta(minutes=dur-1)\n",
    "    end = idx + pd.Timedelta(minutes=1)\n",
    "    block = agg.loc[start:end - pd.Timedelta(seconds=1)]\n",
    "    # Throughput guard\n",
    "    if block['n'].mean() < MIN_COUNT_PER_MIN:\n",
    "        # If throughput is low, relax to MIN_COUNT_PER_MIN/2 or choose next best\n",
    "        print('Throughput low in fallback peak; relaxing guard.')\n",
    "    peak_window = {\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'duration_min': len(block),\n",
    "        'avg_p95': float(block['p95_exec'].mean()),\n",
    "        'max_p95': float(block['p95_exec'].max())\n",
    "    }\n",
    "    print('Fallback PEAK window:', peak_window)\n",
    "\n",
    "# Build candidate baseline windows of same duration (no anomalies, sufficient throughput)\n",
    "dur = peak_window['duration_min']\n",
    "start_all = agg.index.min(); end_all = agg.index.max() + pd.Timedelta(minutes=1)\n",
    "anom_minutes = set(anom.index)\n",
    "\n",
    "best_baseline = None\n",
    "cursor = start_all\n",
    "while cursor + pd.Timedelta(minutes=dur) <= end_all:\n",
    "    w_start = cursor\n",
    "    w_end = cursor + pd.Timedelta(minutes=dur)\n",
    "    minutes = pd.date_range(w_start, w_end - pd.Timedelta(minutes=1), freq='min')  # fix deprecation\n",
    "\n",
    "    # Exclude if any minute is anomalous or overlaps the peak window\n",
    "    if any(m in anom_minutes for m in minutes):\n",
    "        cursor += pd.Timedelta(minutes=1); continue\n",
    "    if not (w_end <= peak_window['start'] or w_start >= peak_window['end']):\n",
    "        cursor += pd.Timedelta(minutes=1); continue\n",
    "\n",
    "    block = agg.loc[w_start:w_end - pd.Timedelta(seconds=1)]\n",
    "    # Coverage & throughput guards\n",
    "    if len(block) < max(5, dur * 0.8):\n",
    "        cursor += pd.Timedelta(minutes=1); continue\n",
    "    if block['n'].mean() < MIN_COUNT_PER_MIN:\n",
    "        cursor += pd.Timedelta(minutes=1); continue\n",
    "\n",
    "    avg_p95 = float(block['p95_exec'].mean())\n",
    "    candidate = {\n",
    "        'start': w_start, 'end': w_end, 'duration_min': len(block),\n",
    "        'avg_p95': avg_p95, 'max_p95': float(block['p95_exec'].max())\n",
    "    }\n",
    "    if (best_baseline is None) or (avg_p95 < best_baseline['avg_p95']):\n",
    "        best_baseline = candidate\n",
    "    cursor += pd.Timedelta(minutes=1)\n",
    "\n",
    "# Final fallback if baseline not found: earliest window of same duration (non-strict)\n",
    "if best_baseline is None:\n",
    "    print(\"No clean baseline found. Using fallback: earliest window of same duration.\")\n",
    "    w_start = agg.index.min()\n",
    "    w_end = w_start + pd.Timedelta(minutes=dur)\n",
    "    block = agg.loc[w_start:w_end - pd.Timedelta(seconds=1)]\n",
    "    best_baseline = {\n",
    "        'start': w_start, 'end': w_end, 'duration_min': len(block),\n",
    "        'avg_p95': float(block['p95_exec'].mean()), 'max_p95': float(block['p95_exec'].max())\n",
    "    }\n",
    "\n",
    "print('Selected BASELINE window:', best_baseline)\n",
    "\n",
    "# Materialize slices\n",
    "baseline_slice = df[(df['datetime'] >= best_baseline['start']) & (df['datetime'] < best_baseline['end'])]\n",
    "peak_slice     = df[(df['datetime'] >= peak_window['start'])   & (df['datetime'] < peak_window['end'])]\n",
    "\n",
    "print('Baseline count:', len(baseline_slice), 'Peak count:', len(peak_slice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf0a972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISCOVERED Baseline metrics:\n",
      "{\n",
      "  \"count\": 272,\n",
      "  \"time_min\": \"2025-11-06 03:23:00.444000\",\n",
      "  \"time_max\": \"2025-11-06 03:27:58.362000\",\n",
      "  \"mean\": 0.8830661764705883,\n",
      "  \"median\": 0.672,\n",
      "  \"p95\": 1.9514499999999992,\n",
      "  \"p99\": 3.672470000000001,\n",
      "  \"std\": 0.6348215439629458,\n",
      "  \"min\": 0.022,\n",
      "  \"max\": 3.885,\n",
      "  \"slow_pct_gt20\": 0.0,\n",
      "  \"critical_pct_gt60\": 0.0\n",
      "}\n",
      "DISCOVERED Peak metrics:\n",
      "{\n",
      "  \"count\": 493,\n",
      "  \"time_min\": \"2025-11-06 22:53:00.592000\",\n",
      "  \"time_max\": \"2025-11-06 22:57:59.922000\",\n",
      "  \"mean\": 15.159847870182555,\n",
      "  \"median\": 12.937,\n",
      "  \"p95\": 50.2886,\n",
      "  \"p99\": 77.91739999999994,\n",
      "  \"std\": 15.241515095849847,\n",
      "  \"min\": 0.029,\n",
      "  \"max\": 89.837,\n",
      "  \"slow_pct_gt20\": 18.864097363083165,\n",
      "  \"critical_pct_gt60\": 3.2454361054766734\n",
      "}\n",
      "Slowdown factor (mean): 17.17x | Increase: 1616.73%\n",
      "Saved -> artifact_discovery_verification.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Metrics & verification on discovered windows\n",
    "\n",
    "def stats(s: pd.DataFrame):\n",
    "    return {\n",
    "        'count': int(len(s)),\n",
    "        'time_min': str(s['datetime'].min()),\n",
    "        'time_max': str(s['datetime'].max()),\n",
    "        'mean': float(s['execution_time'].mean()),\n",
    "        'median': float(s['execution_time'].median()),\n",
    "        'p95': float(s['execution_time'].quantile(0.95)),\n",
    "        'p99': float(s['execution_time'].quantile(0.99)),\n",
    "        'std': float(s['execution_time'].std(ddof=0)),\n",
    "        'min': float(s['execution_time'].min()),\n",
    "        'max': float(s['execution_time'].max()),\n",
    "        'slow_pct_gt20': float((s['execution_time']>20).mean()*100.0),\n",
    "        'critical_pct_gt60': float((s['execution_time']>60).mean()*100.0),\n",
    "    }\n",
    "\n",
    "b = stats(baseline_slice)\n",
    "p = stats(peak_slice)\n",
    "slowdown = p['mean']/b['mean'] if b['mean'] else np.nan\n",
    "increase = (p['mean']-b['mean'])/b['mean']*100 if b['mean'] else np.nan\n",
    "\n",
    "print('DISCOVERED Baseline metrics:')\n",
    "print(json.dumps(b, indent=2))\n",
    "print('DISCOVERED Peak metrics:')\n",
    "print(json.dumps(p, indent=2))\n",
    "print(f\"Slowdown factor (mean): {slowdown:.2f}x | Increase: {increase:.2f}%\")\n",
    "\n",
    "# Save discovery artifact\n",
    "discovery = {\n",
    "    'dataset_sha256': sha,\n",
    "    'discovered_baseline_window': {'start': str(best_baseline['start']), 'end': str(best_baseline['end']), 'duration_min': int(best_baseline['duration_min'])},\n",
    "    'discovered_peak_window': {'start': str(peak_window['start']), 'end': str(peak_window['end']), 'duration_min': int(peak_window['duration_min'])},\n",
    "    'baseline_metrics': b,\n",
    "    'peak_metrics': p,\n",
    "    'slowdown_factor_mean': round(float(slowdown), 3),\n",
    "    'increase_percent_mean': round(float(increase), 2)\n",
    "}\n",
    "open('artifact_discovery_verification.json','w').write(json.dumps(discovery, indent=2))\n",
    "print('Saved -> artifact_discovery_verification.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a49e6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No manual verification artifact found; skipping comparison.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5) Optional comparison to manual windows (if artifact_verification_case.json exists)\n",
    "manual_path = 'artifact_verification_case.json'\n",
    "if os.path.exists(manual_path):\n",
    "    manual = json.load(open(manual_path))\n",
    "    print('Found manual verification artifact; comparing...')\n",
    "    print(json.dumps({\n",
    "        'manual_baseline_window': manual.get('baseline_window'),\n",
    "        'manual_peak_window': manual.get('peak_window'),\n",
    "        'manual_slowdown_factor_mean': manual.get('slowdown_factor_mean'),\n",
    "        'manual_increase_percent_mean': manual.get('increase_percent_mean')\n",
    "    }, indent=2))\n",
    "else:\n",
    "    print('No manual verification artifact found; skipping comparison.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f641fcbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2060091012.py, line 20)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint('Baseline{best_baseline}['start']')\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) Plots: per-minute p95 with anomalies, and ECDF comparison (discovered baseline vs peak)\n",
    "\n",
    "# Per-minute p95 (global) with anomalies\n",
    "fig, ax = plt.subplots(figsize=(11,4))\n",
    "ax.plot(agg.index, agg['p95_exec'], color='#2b8cbe', lw=1.5, label='per-minute p95')\n",
    "anom_plot = agg[agg['robust_z']>3]\n",
    "if not anom_plot.empty:\n",
    "    ax.scatter(anom_plot.index, anom_plot['p95_exec'], s=12, color='red', label='anomaly (z>3)')\n",
    "# highlight discovered windows\n",
    "ax.axvspan(pd.to_datetime(best_baseline['start']), pd.to_datetime(best_baseline['end']), color='#1b7837', alpha=0.15, label='Baseline window')\n",
    "ax.axvspan(pd.to_datetime(peak_window['start']), pd.to_datetime(peak_window['end']), color='#fb6a4a', alpha=0.15, label='Peak window')\n",
    "ax.set_title('Per-minute p95 (global) with discovered windows')\n",
    "ax.set_ylabel('p95 exec (s)')\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%m-%d %H:%M'))\n",
    "ax.legend(loc='upper left', frameon=False)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "fig.savefig('plot_discovery_p95_windows.png', dpi=160)\n",
    "print('=================')\n",
    "print('Baseline{best_baseline}['start']')\n",
    "\n",
    "# ECDF comparison (discovered baseline vs peak)\n",
    "import numpy as np\n",
    "\n",
    "def ecdf(values):\n",
    "    if len(values)==0:\n",
    "        return np.array([]), np.array([])\n",
    "    x = np.sort(values)\n",
    "    y = np.arange(1, len(x)+1)/len(x)\n",
    "    return x, y\n",
    "\n",
    "x_b, y_b = ecdf(baseline_slice['execution_time'].values)\n",
    "x_p, y_p = ecdf(peak_slice['execution_time'].values)\n",
    "\n",
    "THRESH_SLOW=20.0; THRESH_CRIT=60.0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "if len(x_b): ax.plot(x_b, y_b, color='#1b7837', lw=1.8, label='Baseline ECDF')\n",
    "if len(x_p): ax.plot(x_p, y_p, color='#2b8cbe', lw=1.8, label='Peak ECDF')\n",
    "ax.axvline(THRESH_SLOW, color='#cc7a00', lw=1.2, ls='--', label='20s')\n",
    "ax.axvline(THRESH_CRIT, color='#b30000', lw=1.2, ls='--', label='60s')\n",
    "ax.set_title('ECDF comparison — Discovered Baseline vs Peak')\n",
    "ax.set_xlabel('Execution time (s)'); ax.set_ylabel('Cumulative fraction (0–1)')\n",
    "ax.set_xlim(left=0); ax.set_ylim(0,1)\n",
    "ax.grid(True, alpha=0.3); ax.legend(loc='lower right', frameon=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "fig.savefig('plot_discovery_ecdf_baseline_vs_peak.png', dpi=160)\n",
    "\n",
    "print('Saved -> plot_discovery_p95_windows.png, plot_discovery_ecdf_baseline_vs_peak.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2631f6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> artifact_index_discovery.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) Artifact index (discovery)\n",
    "index_payload = {\n",
    "    'dataset': CSV_PATH,\n",
    "    'sha256': sha,\n",
    "    'generated_at': str(datetime.now()),\n",
    "    'artifacts': [\n",
    "        'artifact_discovery_verification.json',\n",
    "        'plot_discovery_p95_windows.png',\n",
    "        'plot_discovery_ecdf_baseline_vs_peak.png'\n",
    "    ]\n",
    "}\n",
    "open('artifact_index_discovery.json','w').write(json.dumps(index_payload, indent=2))\n",
    "print('Saved -> artifact_index_discovery.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe826f2-3583-47dc-85a1-5eba0309c5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
