{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Verification - Incident Analysis Accuracy Check\n",
    "\n",
    "**Verification Objectives:**\n",
    "1. Verify peak slow transaction rate (26.1% at 22:10)\n",
    "2. Cross-check worst transaction times and timestamps\n",
    "3. Validate baseline calculations and degradation percentages\n",
    "4. Confirm incident timeline accuracy\n",
    "5. Ensure all claims are evidence-based\n",
    "\n",
    "**Data Sources:**\n",
    "- time6.txt: 6 Nov 2025 logs\n",
    "- time.txt: 7 Nov 2025 logs\n",
    "- Analysis period: 21:30 (6 Nov) ‚Üí 03:30 (7 Nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evidence Verification Environment Ready!\n",
      " Checking accuracy of all incident analysis claims...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "print(\" Evidence Verification Environment Ready!\")\n",
    "print(\" Checking accuracy of all incident analysis claims...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading time6.txt with verification...\n",
      "  Loaded 141,657 records\n",
      " Loading time.txt with verification...\n",
      "  Loaded 16,529 records\n",
      "\n",
      " VERIFICATION DATASET SUMMARY:\n",
      "  Total records: 158,186\n",
      "  Time range: 2025-11-06 00:00:00.850000 ‚Üí 2025-11-07 03:33:49.509000\n",
      "  Data sources: {'time6.txt': 141657, 'time.txt': 16529}\n"
     ]
    }
   ],
   "source": [
    "def parse_verification_logs(filepath, is_time6=True):\n",
    "    \"\"\"\n",
    "    Parse logs with detailed verification tracking\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    incident_time = datetime(2025, 11, 6, 22, 10)\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        for line_num, line in enumerate(file, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Extract components with validation\n",
    "            pattern = r'(eta_\\w+)\\.(\\d+):([\\d/]+) ([\\d:.]+) (\\d+) TOOK ([\\d.]+)s'\n",
    "            match = re.match(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                agent_type, pid, date, time, transaction_id, execution_time = match.groups()\n",
    "                \n",
    "                try:\n",
    "                    # Parse datetime with error handling\n",
    "                    dt = datetime.strptime(f\"{date} {time}\", \"%d/%m/%Y %H:%M:%S.%f\")\n",
    "                    exec_time = float(execution_time)\n",
    "                    \n",
    "                    # Store with verification metadata\n",
    "                    data.append({\n",
    "                        'line_number': line_num,\n",
    "                        'source_file': 'time6.txt' if is_time6 else 'time.txt',\n",
    "                        'agent_type': agent_type,\n",
    "                        'pid': int(pid),\n",
    "                        'datetime': dt,\n",
    "                        'date': dt.date(),\n",
    "                        'hour': dt.hour,\n",
    "                        'minute': dt.minute,\n",
    "                        'transaction_id': int(transaction_id),\n",
    "                        'execution_time': exec_time,\n",
    "                        'raw_line': line,\n",
    "                        'is_slow': exec_time > 20,\n",
    "                        'is_very_slow': exec_time > 30,\n",
    "                        'is_critical': exec_time > 60\n",
    "                    })\n",
    "                except ValueError as e:\n",
    "                    print(f\" Parse error at line {line_num}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load both files with verification\n",
    "print(\" Loading time6.txt with verification...\")\n",
    "df_time6_verify = parse_verification_logs('time6.txt', is_time6=True)\n",
    "print(f\"Loaded {len(df_time6_verify):,} records\")\n",
    "\n",
    "print(\"Loading time.txt with verification...\")\n",
    "df_time7_verify = parse_verification_logs('time.txt', is_time6=False)\n",
    "print(f\"  Loaded {len(df_time7_verify):,} records\")\n",
    "\n",
    "# Combine and sort\n",
    "df_verify = pd.concat([df_time6_verify, df_time7_verify], ignore_index=True)\n",
    "df_verify = df_verify.sort_values('datetime')\n",
    "\n",
    "print(f\"\\n VERIFICATION DATASET SUMMARY:\")\n",
    "print(f\"  Total records: {len(df_verify):,}\")\n",
    "print(f\"  Time range: {df_verify['datetime'].min()} ‚Üí {df_verify['datetime'].max()}\")\n",
    "print(f\"  Data sources: {df_verify['source_file'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Peak Slow Transaction Rate Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION 1: Peak Slow Transaction Rate\n",
      "==================================================\n",
      "MINUTE-BY-MINUTE ANALYSIS (around 22:10):\n",
      "                     total_txns  avg_time  slow_count  slow_rate  \\\n",
      "minute_mark                                                        \n",
      "2025-11-06 21:40:00          80     3.175           0      0.000   \n",
      "2025-11-06 21:41:00          83     1.652           0      0.000   \n",
      "2025-11-06 21:42:00         104     2.169           0      0.000   \n",
      "2025-11-06 21:43:00         154     1.004           0      0.000   \n",
      "2025-11-06 21:44:00          97     1.063           0      0.000   \n",
      "2025-11-06 21:45:00         102     4.049           1      0.010   \n",
      "2025-11-06 21:46:00          69     0.984           0      0.000   \n",
      "2025-11-06 21:47:00         103     5.366           3      0.029   \n",
      "2025-11-06 21:48:00         108     1.500           0      0.000   \n",
      "2025-11-06 21:49:00          79     1.229           0      0.000   \n",
      "2025-11-06 21:50:00         100     0.861           0      0.000   \n",
      "2025-11-06 21:51:00          86     0.984           0      0.000   \n",
      "2025-11-06 21:52:00          87     1.507           0      0.000   \n",
      "2025-11-06 21:53:00          94     1.328           0      0.000   \n",
      "2025-11-06 21:54:00          95     2.221           0      0.000   \n",
      "2025-11-06 21:55:00          84     2.067           0      0.000   \n",
      "2025-11-06 21:56:00          84     4.683           3      0.036   \n",
      "2025-11-06 21:57:00         107     0.986           0      0.000   \n",
      "2025-11-06 21:58:00          85     0.688           0      0.000   \n",
      "2025-11-06 21:59:00          83     1.058           0      0.000   \n",
      "\n",
      "                     critical_count  slow_percentage  \n",
      "minute_mark                                           \n",
      "2025-11-06 21:40:00               0              0.0  \n",
      "2025-11-06 21:41:00               0              0.0  \n",
      "2025-11-06 21:42:00               0              0.0  \n",
      "2025-11-06 21:43:00               0              0.0  \n",
      "2025-11-06 21:44:00               0              0.0  \n",
      "2025-11-06 21:45:00               0              1.0  \n",
      "2025-11-06 21:46:00               0              0.0  \n",
      "2025-11-06 21:47:00               0              2.9  \n",
      "2025-11-06 21:48:00               0              0.0  \n",
      "2025-11-06 21:49:00               0              0.0  \n",
      "2025-11-06 21:50:00               0              0.0  \n",
      "2025-11-06 21:51:00               0              0.0  \n",
      "2025-11-06 21:52:00               0              0.0  \n",
      "2025-11-06 21:53:00               0              0.0  \n",
      "2025-11-06 21:54:00               0              0.0  \n",
      "2025-11-06 21:55:00               0              0.0  \n",
      "2025-11-06 21:56:00               0              3.6  \n",
      "2025-11-06 21:57:00               0              0.0  \n",
      "2025-11-06 21:58:00               0              0.0  \n",
      "2025-11-06 21:59:00               0              0.0  \n",
      "\n",
      "ACTUAL PEAK SLOW RATE VERIFICATION:\n",
      "  Peak slow rate: 62.1% at 22:24\n",
      "  Evidence: 72 slow out of 116 total transactions\n",
      "  Raw calculation: 72/116 = 62.1%\n",
      "\n",
      "22:10 SPECIFIC VERIFICATION:\n",
      "22:10 slow rate: 15.7%\n",
      "  Evidence: 24 slow out of 153 transactions at 22:10\n",
      "\n",
      "CLAIM VERIFICATION RESULT:\n",
      " DISCREPANCY: Peak rate 62.1% ‚â† claimed 26.1%\n",
      " Difference: 36.0 percentage points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/nix-shell.aQbSAD/ipykernel_53944/247743676.py:16: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  incident_window['minute_mark'] = incident_window['datetime'].dt.floor('T')\n"
     ]
    }
   ],
   "source": [
    "# CLAIM: Peak slow transaction rate: 26.1% at 22:10\n",
    "print(\"VERIFICATION 1: Peak Slow Transaction Rate\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check each minute around 22:10 for accuracy\n",
    "target_time = datetime(2025, 11, 6, 22, 10)\n",
    "time_window = timedelta(minutes=30)  # Check ¬±30 minutes\n",
    "\n",
    "# Filter data around incident time\n",
    "incident_window = df_verify[\n",
    "    (df_verify['datetime'] >= target_time - time_window) &\n",
    "    (df_verify['datetime'] <= target_time + time_window)\n",
    "].copy()\n",
    "\n",
    "# Group by minute and calculate slow percentages\n",
    "incident_window['minute_mark'] = incident_window['datetime'].dt.floor('T')\n",
    "minute_analysis = incident_window.groupby('minute_mark').agg({\n",
    "    'execution_time': ['count', 'mean'],\n",
    "    'is_slow': ['sum', 'mean'],\n",
    "    'is_critical': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "minute_analysis.columns = ['total_txns', 'avg_time', 'slow_count', 'slow_rate', 'critical_count']\n",
    "minute_analysis['slow_percentage'] = minute_analysis['slow_rate'] * 100\n",
    "\n",
    "print(\"MINUTE-BY-MINUTE ANALYSIS (around 22:10):\")\n",
    "print(minute_analysis[minute_analysis['total_txns'] > 0].head(20))\n",
    "\n",
    "# Find actual peak slow percentage\n",
    "valid_minutes = minute_analysis[minute_analysis['total_txns'] >= 10]  # At least 10 transactions\n",
    "peak_slow_minute = valid_minutes['slow_percentage'].idxmax()\n",
    "peak_slow_rate = valid_minutes.loc[peak_slow_minute, 'slow_percentage']\n",
    "peak_total_txns = valid_minutes.loc[peak_slow_minute, 'total_txns']\n",
    "peak_slow_count = valid_minutes.loc[peak_slow_minute, 'slow_count']\n",
    "\n",
    "print(f\"\\n ACTUAL PEAK SLOW RATE VERIFICATION:\")\n",
    "print(f\"  Peak slow rate: {peak_slow_rate:.1f}% at {peak_slow_minute.strftime('%H:%M')}\")\n",
    "print(f\"  Evidence: {peak_slow_count:.0f} slow out of {peak_total_txns:.0f} total transactions\")\n",
    "print(f\"  Raw calculation: {peak_slow_count}/{peak_total_txns} = {peak_slow_rate:.1f}%\")\n",
    "\n",
    "# Check specifically 22:10\n",
    "exact_2210 = minute_analysis.loc[minute_analysis.index == datetime(2025, 11, 6, 22, 10)]\n",
    "if len(exact_2210) > 0:\n",
    "    rate_2210 = exact_2210['slow_percentage'].iloc[0]\n",
    "    total_2210 = exact_2210['total_txns'].iloc[0]\n",
    "    slow_2210 = exact_2210['slow_count'].iloc[0]\n",
    "    print(f\"\\n 22:10 SPECIFIC VERIFICATION:\")\n",
    "    print(f\"  22:10 slow rate: {rate_2210:.1f}%\")\n",
    "    print(f\"  Evidence: {slow_2210:.0f} slow out of {total_2210:.0f} transactions at 22:10\")\n",
    "else:\n",
    "    print(f\"\\n No data found for exactly 22:10\")\n",
    "\n",
    "# Verify if claim matches evidence\n",
    "print(f\"\\n CLAIM VERIFICATION RESULT:\")\n",
    "if abs(peak_slow_rate - 26.1) < 1.0:  # Within 1% tolerance\n",
    "    print(f\"  VERIFIED: Peak rate {peak_slow_rate:.1f}% ‚âà claimed 26.1%\")\n",
    "else:\n",
    "    print(f\"   DISCREPANCY: Peak rate {peak_slow_rate:.1f}% ‚â† claimed 26.1%\")\n",
    "    print(f\"   Difference: {abs(peak_slow_rate - 26.1):.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Worst Transaction Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION 2: Worst Transaction Times\n",
      "==================================================\n",
      "ACTUAL TOP 15 WORST TRANSACTIONS (from our data):\n",
      "14:11:30 | PID 8079 | 119.2s | TXN 11017738838699 | time6.txt:105236\n",
      "14:11:30 | PID 7695 | 118.8s | TXN 11017738838700 | time6.txt:12398\n",
      "14:11:48 | PID 7861 | 116.8s | TXN 11017738845216 | time6.txt:70410\n",
      "14:10:00 | PID 7985 | 116.5s | TXN 11017738837744 | time6.txt:92285\n",
      "14:10:09 | PID 8023 | 116.2s | TXN 11017738837829 | time6.txt:94465\n",
      "22:25:27 | PID 7860 | 115.9s | TXN 11017739576934 | time6.txt:69043\n",
      "22:25:19 | PID 7865 | 115.3s | TXN 11017739585840 | time6.txt:75621\n",
      "22:25:17 | PID 7766 | 113.8s | TXN 11017739585838 | time6.txt:31025\n",
      "22:18:33 | PID 7844 | 111.3s | TXN 11017739567853 | time6.txt:57735\n",
      "14:11:53 | PID 8097 | 109.6s | TXN 11017738840609 | time6.txt:116173\n",
      "14:09:03 | PID 7692 | 104.9s | TXN 11017738837247 | time6.txt:5618\n",
      "23:04:43 | PID 8087 | 104.9s | TXN 11017739623549 | time6.txt:110474\n",
      "22:25:06 | PID 8097 | 103.1s | TXN 11017739576894 | time6.txt:117106\n",
      "22:25:06 | PID 7692 | 102.8s | TXN 11017739576896 | time6.txt:6581\n",
      "22:16:51 | PID 8144 | 101.5s | TXN 11017739567163 | time6.txt:130253\n",
      "22:16:51 | PID 7763 | 101.4s | TXN 11017739570903 | time6.txt:24482\n",
      "22:16:50 | PID 8025 | 100.6s | TXN 11017739570902 | time6.txt:99727\n",
      "22:16:13 | PID 7961 | 98.7s | TXN 11017739566904 | time6.txt:77817\n",
      "22:16:48 | PID 8098 | 98.5s | TXN 11017739570900 | time6.txt:119245\n",
      "22:25:01 | PID 7852 | 97.9s | TXN 11017739585836 | time6.txt:64432\n",
      "22:16:48 | PID 8084 | 97.8s | TXN 11017739570907 | time6.txt:108273\n",
      "22:17:52 | PID 7765 | 97.6s | TXN 11017739571263 | time6.txt:28767\n",
      "23:04:35 | PID 7850 | 97.0s | TXN 11017739623556 | time6.txt:60020\n",
      "23:04:35 | PID 7773 | 96.8s | TXN 11017739631662 | time6.txt:46642\n",
      "23:04:35 | PID 7693 | 96.7s | TXN 11017739631661 | time6.txt:8906\n",
      "23:04:35 | PID 8084 | 96.3s | TXN 11017739618129 | time6.txt:108349\n",
      "22:17:10 | PID 7860 | 96.1s | TXN 11017739575029 | time6.txt:69032\n",
      "22:16:46 | PID 7851 | 95.9s | TXN 11017739570904 | time6.txt:62177\n",
      "00:04:38 | PID 7764 | 94.8s | TXN 11017739713079 | time.txt:2814\n",
      "14:10:34 | PID 8087 | 93.5s | TXN 11017738824896 | time6.txt:109547\n",
      "22:18:14 | PID 7793 | 93.5s | TXN 11017739567841 | time6.txt:53229\n",
      "14:11:17 | PID 7773 | 93.4s | TXN 11017738838837 | time6.txt:45714\n",
      "22:16:43 | PID 7693 | 93.3s | TXN 11017739570906 | time6.txt:8829\n",
      "22:16:43 | PID 8093 | 92.8s | TXN 11017739567164 | time6.txt:114853\n",
      "22:25:04 | PID 8098 | 92.7s | TXN 11017739576935 | time6.txt:119259\n",
      "22:16:42 | PID 7695 | 92.5s | TXN 11017739567161 | time6.txt:13311\n",
      "00:08:30 | PID 7695 | 92.1s | TXN 11017739715152 | time.txt:1253\n",
      "00:07:54 | PID 8079 | 91.9s | TXN 11017739714817 | time.txt:12147\n",
      "22:24:55 | PID 7985 | 91.3s | TXN 11017739585842 | time6.txt:93171\n",
      "22:10:22 | PID 8079 | 91.1s | TXN 11017739558865 | time6.txt:106146\n",
      "00:04:53 | PID 7785 | 90.9s | TXN 11017739713235 | time.txt:5448\n",
      "22:55:00 | PID 8097 | 89.8s | TXN 11017739604187 | time6.txt:117144\n",
      "14:15:04 | PID 7793 | 89.8s | TXN 11017738834903 | time6.txt:52378\n",
      "00:02:06 | PID 7764 | 89.6s | TXN 11017739701676 | time.txt:2811\n",
      "22:19:01 | PID 7695 | 89.1s | TXN 11017739571677 | time6.txt:13313\n",
      "22:24:52 | PID 7844 | 89.1s | TXN 11017739585845 | time6.txt:57741\n",
      "22:24:52 | PID 7765 | 89.1s | TXN 11017739585841 | time6.txt:28774\n",
      "23:07:56 | PID 7964 | 89.0s | TXN 11017739632893 | time6.txt:84611\n",
      "23:04:44 | PID 7844 | 88.8s | TXN 11017739626506 | time6.txt:57791\n",
      "22:14:57 | PID 8084 | 88.7s | TXN 11017739549519 | time6.txt:108272\n",
      "\n",
      " VERIFYING CLAIMED WORST TRANSACTIONS:\n",
      "   Claim 1: PID 7860, 115.9s\n",
      "    Found: 22:25:27, 115.9s, TXN 11017739576934\n",
      "   Claim 2: PID 7865, 115.3s\n",
      "    Found: 22:25:19, 115.3s, TXN 11017739585840\n",
      "   Claim 3: PID 7766, 113.8s\n",
      "    Found: 22:25:17, 113.8s, TXN 11017739585838\n",
      "   Claim 4: PID 7844, 111.3s\n",
      "    Found: 22:18:33, 111.3s, TXN 11017739567853\n",
      "   Claim 5: PID 8144, 101.5s\n",
      "      Found: 22:16:51, 101.5s, TXN 11017739567163\n",
      "\n",
      " WORST TRANSACTION SUMMARY:\n",
      "  Actual worst: 119.2s at 14:11:30\n",
      "  Claimed worst: 115.9s at 22:25:27\n",
      "   DISCREPANCY: Different worst transaction found\n"
     ]
    }
   ],
   "source": [
    "# CLAIM: Top worst transactions with specific times and PIDs\n",
    "print(\" VERIFICATION 2: Worst Transaction Times\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get actual top 15 worst transactions from our data\n",
    "worst_actual = df_verify.nlargest(50, 'execution_time')[[\n",
    "    'datetime', 'pid', 'execution_time', 'transaction_id', \n",
    "    'source_file', 'line_number'\n",
    "]].copy()\n",
    "\n",
    "worst_actual['time_str'] = worst_actual['datetime'].dt.strftime('%H:%M:%S')\n",
    "worst_actual['date_str'] = worst_actual['datetime'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"ACTUAL TOP 15 WORST TRANSACTIONS (from our data):\")\n",
    "for idx, row in worst_actual.iterrows():\n",
    "    print(f\"{row['time_str']} | PID {row['pid']} | {row['execution_time']:.1f}s | TXN {row['transaction_id']} | {row['source_file']}:{row['line_number']}\")\n",
    "\n",
    "# Verify specific claimed transactions\n",
    "claimed_worst = [\n",
    "    {'time': '22:25:27', 'pid': 7860, 'seconds': 115.9, 'txn': 11017739576934},\n",
    "    {'time': '22:25:19', 'pid': 7865, 'seconds': 115.3, 'txn': 11017739585840},\n",
    "    {'time': '22:25:17', 'pid': 7766, 'seconds': 113.8, 'txn': 11017739585838},\n",
    "    {'time': '22:18:33', 'pid': 7844, 'seconds': 111.3, 'txn': 11017739567853},\n",
    "    {'time': '22:16:51', 'pid': 8144, 'seconds': 101.5, 'txn': 11017739567163}\n",
    "]\n",
    "\n",
    "print(f\"\\n VERIFYING CLAIMED WORST TRANSACTIONS:\")\n",
    "for i, claim in enumerate(claimed_worst, 1):\n",
    "    # Search for exact match\n",
    "    matches = df_verify[\n",
    "        (df_verify['pid'] == claim['pid']) &\n",
    "        (df_verify['transaction_id'] == claim['txn']) &\n",
    "        (abs(df_verify['execution_time'] - claim['seconds']) < 0.1)\n",
    "    ]\n",
    "    \n",
    "    if len(matches) > 0:\n",
    "        match = matches.iloc[0]\n",
    "        actual_time = match['datetime'].strftime('%H:%M:%S')\n",
    "        print(f\"   Claim {i}: PID {claim['pid']}, {claim['seconds']}s\")\n",
    "        print(f\"     Found: {actual_time}, {match['execution_time']:.1f}s, TXN {match['transaction_id']}\")\n",
    "    else:\n",
    "        # Look for close matches\n",
    "        close_pid = df_verify[df_verify['pid'] == claim['pid']]\n",
    "        close_txn = df_verify[df_verify['transaction_id'] == claim['txn']]\n",
    "        close_time = df_verify[abs(df_verify['execution_time'] - claim['seconds']) < 1.0]\n",
    "        \n",
    "        print(f\"   Claim {i}: PID {claim['pid']}, {claim['seconds']}s - NOT FOUND\")\n",
    "        print(f\"      Similar PID transactions: {len(close_pid)}\")\n",
    "        print(f\"      Similar TXN IDs: {len(close_txn)}\")\n",
    "        print(f\"      Similar execution times: {len(close_time)}\")\n",
    "\n",
    "# Summary verification\n",
    "print(f\"\\n WORST TRANSACTION SUMMARY:\")\n",
    "print(f\"  Actual worst: {worst_actual.iloc[0]['execution_time']:.1f}s at {worst_actual.iloc[0]['time_str']}\")\n",
    "print(f\"  Claimed worst: 115.9s at 22:25:27\")\n",
    "if abs(worst_actual.iloc[0]['execution_time'] - 115.9) < 1.0:\n",
    "    print(f\"   VERIFIED: Worst transaction time matches claim\")\n",
    "else:\n",
    "    print(f\"   DISCREPANCY: Different worst transaction found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Baseline and Degradation Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFICATION 3: Baseline & Degradation Calculations\n",
      "=======================================================\n",
      "üìä BASELINE PERIOD VERIFICATION (21:30-22:00):\n",
      "  Total transactions: 2,756\n",
      "  Time range: 2025-11-06 21:30:04.074000 ‚Üí 2025-11-06 21:59:59.654000\n",
      "  Mean: 2.027 seconds\n",
      "  Median: 0.901 seconds\n",
      "  P95: 7.342 seconds\n",
      "  Std Dev: 2.880 seconds\n",
      "  Slow rate (>20s): 0.40%\n",
      "  Min: 0.027s\n",
      "  Max: 34.083s\n",
      "\n",
      "üìä PEAK PERIOD VERIFICATION (22:20-23:10):\n",
      "  Total transactions: 4,708\n",
      "  Time range: 2025-11-06 22:20:00.187000 ‚Üí 2025-11-06 23:09:59.259000\n",
      "  Mean: 13.187 seconds\n",
      "  Median: 11.792 seconds\n",
      "  P95: 35.747 seconds\n",
      "  Std Dev: 13.160 seconds\n",
      "  Slow rate (>20s): 16.91%\n",
      "  Critical rate (>60s): 1.70%\n",
      "  Min: 0.021s\n",
      "  Max: 115.895s\n",
      "\n",
      "üîé DEGRADATION VERIFICATION:\n",
      "  Baseline mean: 2.027s\n",
      "  Peak mean: 13.187s\n",
      "  Degradation: +550.6%\n",
      "  Slowdown factor: 6.5x\n",
      "\n",
      "üìã CLAIM VERIFICATION:\n",
      "  ‚úÖ Baseline ~2.0s: VERIFIED (2.03s)\n",
      "  ‚úÖ Peak ~13.2s: VERIFIED (13.19s)\n",
      "  ‚úÖ 6.5x slowdown: VERIFIED (6.5x)\n"
     ]
    }
   ],
   "source": [
    "# CLAIM: Baseline ~2.0s, Peak ~13.2s (6.5x degradation)\n",
    "print(\"üîç VERIFICATION 3: Baseline & Degradation Calculations\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Define time periods precisely\n",
    "baseline_start = datetime(2025, 11, 6, 21, 30)\n",
    "baseline_end = datetime(2025, 11, 6, 22, 0)\n",
    "peak_start = datetime(2025, 11, 6, 22, 20)\n",
    "peak_end = datetime(2025, 11, 6, 23, 10)\n",
    "\n",
    "# Extract baseline data\n",
    "baseline_data = df_verify[\n",
    "    (df_verify['datetime'] >= baseline_start) &\n",
    "    (df_verify['datetime'] < baseline_end)\n",
    "]\n",
    "\n",
    "# Extract peak data\n",
    "peak_data = df_verify[\n",
    "    (df_verify['datetime'] >= peak_start) &\n",
    "    (df_verify['datetime'] < peak_end)\n",
    "]\n",
    "\n",
    "print(f\" BASELINE PERIOD VERIFICATION (21:30-22:00):\")\n",
    "print(f\"  Total transactions: {len(baseline_data):,}\")\n",
    "print(f\"  Time range: {baseline_data['datetime'].min()} ‚Üí {baseline_data['datetime'].max()}\")\n",
    "\n",
    "if len(baseline_data) > 0:\n",
    "    baseline_mean = baseline_data['execution_time'].mean()\n",
    "    baseline_median = baseline_data['execution_time'].median()\n",
    "    baseline_p95 = baseline_data['execution_time'].quantile(0.95)\n",
    "    baseline_slow_pct = (baseline_data['execution_time'] > 20).mean() * 100\n",
    "    baseline_std = baseline_data['execution_time'].std()\n",
    "    \n",
    "    print(f\"  Mean: {baseline_mean:.3f} seconds\")\n",
    "    print(f\"  Median: {baseline_median:.3f} seconds\")\n",
    "    print(f\"  P95: {baseline_p95:.3f} seconds\")\n",
    "    print(f\"  Std Dev: {baseline_std:.3f} seconds\")\n",
    "    print(f\"  Slow rate (>20s): {baseline_slow_pct:.2f}%\")\n",
    "    \n",
    "    # Show distribution\n",
    "    print(f\"  Min: {baseline_data['execution_time'].min():.3f}s\")\n",
    "    print(f\"  Max: {baseline_data['execution_time'].max():.3f}s\")\n",
    "else:\n",
    "    print(f\"   No baseline data found\")\n",
    "\n",
    "print(f\"\\n PEAK PERIOD VERIFICATION (22:20-23:10):\")\n",
    "print(f\"  Total transactions: {len(peak_data):,}\")\n",
    "print(f\"  Time range: {peak_data['datetime'].min()} ‚Üí {peak_data['datetime'].max()}\")\n",
    "\n",
    "if len(peak_data) > 0:\n",
    "    peak_mean = peak_data['execution_time'].mean()\n",
    "    peak_median = peak_data['execution_time'].median()\n",
    "    peak_p95 = peak_data['execution_time'].quantile(0.95)\n",
    "    peak_slow_pct = (peak_data['execution_time'] > 20).mean() * 100\n",
    "    peak_critical_pct = (peak_data['execution_time'] > 60).mean() * 100\n",
    "    peak_std = peak_data['execution_time'].std()\n",
    "    \n",
    "    print(f\"  Mean: {peak_mean:.3f} seconds\")\n",
    "    print(f\"  Median: {peak_median:.3f} seconds\")\n",
    "    print(f\"  P95: {peak_p95:.3f} seconds\")\n",
    "    print(f\"  Std Dev: {peak_std:.3f} seconds\")\n",
    "    print(f\"  Slow rate (>20s): {peak_slow_pct:.2f}%\")\n",
    "    print(f\"  Critical rate (>60s): {peak_critical_pct:.2f}%\")\n",
    "    \n",
    "    # Show distribution\n",
    "    print(f\"  Min: {peak_data['execution_time'].min():.3f}s\")\n",
    "    print(f\"  Max: {peak_data['execution_time'].max():.3f}s\")\n",
    "else:\n",
    "    print(f\"   No peak data found\")\n",
    "\n",
    "# Calculate verified degradation\n",
    "if len(baseline_data) > 0 and len(peak_data) > 0:\n",
    "    mean_degradation = ((peak_mean - baseline_mean) / baseline_mean) * 100\n",
    "    degradation_factor = peak_mean / baseline_mean\n",
    "    \n",
    "    print(f\"\\n DEGRADATION VERIFICATION:\")\n",
    "    print(f\"  Baseline mean: {baseline_mean:.3f}s\")\n",
    "    print(f\"  Peak mean: {peak_mean:.3f}s\")\n",
    "    print(f\"  Degradation: {mean_degradation:+.1f}%\")\n",
    "    print(f\"  Slowdown factor: {degradation_factor:.1f}x\")\n",
    "    \n",
    "    # Verify claims\n",
    "    print(f\"\\nüìã CLAIM VERIFICATION:\")\n",
    "    if abs(baseline_mean - 2.0) < 0.5:\n",
    "        print(f\"  Baseline ~2.0s: VERIFIED ({baseline_mean:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"  Baseline ~2.0s: INCORRECT (actual: {baseline_mean:.2f}s)\")\n",
    "        \n",
    "    if abs(peak_mean - 13.2) < 1.0:\n",
    "        print(f\"   Peak ~13.2s: VERIFIED ({peak_mean:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Peak ~13.2s: INCORRECT (actual: {peak_mean:.2f}s)\")\n",
    "        \n",
    "    if abs(degradation_factor - 6.5) < 1.0:\n",
    "        print(f\"  ‚úÖ 6.5x slowdown: VERIFIED ({degradation_factor:.1f}x)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå 6.5x slowdown: INCORRECT (actual: {degradation_factor:.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Timeline and Period Classification Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFICATION 4: Timeline and Period Classification\n",
      "=======================================================\n",
      "\n",
      "üìä Pre-incident (21:30-22:00):\n",
      "  Transactions: 2,756\n",
      "  Mean: 2.03s\n",
      "  P95: 7.34s\n",
      "  Slow rate: 0.4%\n",
      "\n",
      "üìä Incident Start (22:00-22:20):\n",
      "  Transactions: 2,119\n",
      "  Mean: 11.28s\n",
      "  P95: 34.13s\n",
      "  Slow rate: 14.9%\n",
      "  Critical rate: 1.9%\n",
      "\n",
      "üìä Peak Impact (22:20-23:10):\n",
      "  Transactions: 4,708\n",
      "  Mean: 13.19s\n",
      "  P95: 35.75s\n",
      "  Slow rate: 16.9%\n",
      "  Critical rate: 1.7%\n",
      "\n",
      "üìä Initial Recovery (23:10-00:10):\n",
      "  Transactions: 6,027\n",
      "  Mean: 10.88s\n",
      "  P95: 24.59s\n",
      "  Slow rate: 10.5%\n",
      "  Critical rate: 0.7%\n",
      "\n",
      "üìä Mid Recovery (00:10-02:10):\n",
      "  Transactions: 10,059\n",
      "  Mean: 6.92s\n",
      "  P95: 20.16s\n",
      "  Slow rate: 5.1%\n",
      "  Critical rate: 0.1%\n",
      "\n",
      "üìä Late Recovery (02:10+):\n",
      "  Transactions: 5,149\n",
      "  Mean: 1.28s\n",
      "  P95: 4.39s\n",
      "  Slow rate: 0.1%\n",
      "\n",
      "üïê KEY TIMELINE VERIFICATION:\n",
      "  Reported incident time: 22:10 UTC+3 (19:10 UTC)\n",
      "  Data coverage: 2025-11-06 00:00:00.850000 ‚Üí 2025-11-07 03:33:49.509000\n",
      "  Analysis duration: 27.6 hours\n",
      "  Data at incident time (22:10): 153 transactions\n",
      "  Avg performance at 22:10: 13.19s\n",
      "\n",
      "üîé DATA CONTINUITY CHECK:\n",
      "Hourly transaction counts:\n",
      "  2025-11-06 00:00: 4,840 transactions\n",
      "  2025-11-06 01:00: 3,908 transactions\n",
      "  2025-11-06 02:00: 4,153 transactions\n",
      "  2025-11-06 03:00: 3,685 transactions\n",
      "  2025-11-06 04:00: 3,614 transactions\n",
      "  2025-11-06 05:00: 4,003 transactions\n",
      "  2025-11-06 06:00: 4,960 transactions\n",
      "  2025-11-06 07:00: 5,713 transactions\n",
      "  2025-11-06 08:00: 5,990 transactions\n",
      "  2025-11-06 09:00: 6,268 transactions\n",
      "  2025-11-06 10:00: 6,256 transactions\n",
      "  2025-11-06 11:00: 6,737 transactions\n",
      "  2025-11-06 12:00: 6,805 transactions\n",
      "  2025-11-06 13:00: 7,100 transactions\n",
      "  2025-11-06 14:00: 7,700 transactions\n",
      "  2025-11-06 15:00: 8,054 transactions\n",
      "  2025-11-06 16:00: 7,616 transactions\n",
      "  2025-11-06 17:00: 6,558 transactions\n",
      "  2025-11-06 18:00: 7,267 transactions\n",
      "  2025-11-06 19:00: 6,647 transactions\n",
      "  2025-11-06 20:00: 6,095 transactions\n",
      "  2025-11-06 21:00: 5,905 transactions\n",
      "  2025-11-06 22:00: 5,896 transactions\n",
      "  2025-11-06 23:00: 5,887 transactions\n",
      "  2025-11-07 00:00: 5,777 transactions\n",
      "  2025-11-07 01:00: 4,644 transactions\n",
      "  2025-11-07 02:00: 3,981 transactions\n",
      "  2025-11-07 03:00: 2,127 transactions\n",
      "\n",
      "‚úÖ No significant data gaps detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/nix-shell.aQbSAD/ipykernel_53944/3221536718.py:68: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_counts = df_verify.groupby(df_verify['datetime'].dt.floor('H')).size()\n"
     ]
    }
   ],
   "source": [
    "# Verify our period classifications and timeline\n",
    "print(\"üîç VERIFICATION 4: Timeline and Period Classification\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Define all periods with exact boundaries\n",
    "periods = {\n",
    "    'Pre-incident (21:30-22:00)': (datetime(2025, 11, 6, 21, 30), datetime(2025, 11, 6, 22, 0)),\n",
    "    'Incident Start (22:00-22:20)': (datetime(2025, 11, 6, 22, 0), datetime(2025, 11, 6, 22, 20)),\n",
    "    'Peak Impact (22:20-23:10)': (datetime(2025, 11, 6, 22, 20), datetime(2025, 11, 6, 23, 10)),\n",
    "    'Initial Recovery (23:10-00:10)': (datetime(2025, 11, 6, 23, 10), datetime(2025, 11, 7, 0, 10)),\n",
    "    'Mid Recovery (00:10-02:10)': (datetime(2025, 11, 7, 0, 10), datetime(2025, 11, 7, 2, 10)),\n",
    "    'Late Recovery (02:10+)': (datetime(2025, 11, 7, 2, 10), datetime(2025, 11, 7, 3, 30))\n",
    "}\n",
    "\n",
    "# Analyze each period\n",
    "period_stats = {}\n",
    "for period_name, (start_time, end_time) in periods.items():\n",
    "    period_data = df_verify[\n",
    "        (df_verify['datetime'] >= start_time) &\n",
    "        (df_verify['datetime'] < end_time)\n",
    "    ]\n",
    "    \n",
    "    if len(period_data) > 0:\n",
    "        stats = {\n",
    "            'count': len(period_data),\n",
    "            'mean': period_data['execution_time'].mean(),\n",
    "            'median': period_data['execution_time'].median(),\n",
    "            'p95': period_data['execution_time'].quantile(0.95),\n",
    "            'slow_pct': (period_data['execution_time'] > 20).mean() * 100,\n",
    "            'critical_pct': (period_data['execution_time'] > 60).mean() * 100,\n",
    "            'start': start_time,\n",
    "            'end': end_time\n",
    "        }\n",
    "        period_stats[period_name] = stats\n",
    "        \n",
    "        print(f\"\\nüìä {period_name}:\")\n",
    "        print(f\"  Transactions: {stats['count']:,}\")\n",
    "        print(f\"  Mean: {stats['mean']:.2f}s\")\n",
    "        print(f\"  P95: {stats['p95']:.2f}s\")\n",
    "        print(f\"  Slow rate: {stats['slow_pct']:.1f}%\")\n",
    "        if stats['critical_pct'] > 0:\n",
    "            print(f\"  Critical rate: {stats['critical_pct']:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è {period_name}: No data found\")\n",
    "\n",
    "# Key timeline verification\n",
    "print(f\"\\nüïê KEY TIMELINE VERIFICATION:\")\n",
    "incident_start = datetime(2025, 11, 6, 22, 10)\n",
    "first_data = df_verify['datetime'].min()\n",
    "last_data = df_verify['datetime'].max()\n",
    "\n",
    "print(f\"  Reported incident time: 22:10 UTC+3 (19:10 UTC)\")\n",
    "print(f\"  Data coverage: {first_data} ‚Üí {last_data}\")\n",
    "print(f\"  Analysis duration: {(last_data - first_data).total_seconds() / 3600:.1f} hours\")\n",
    "\n",
    "# Verify data around incident time\n",
    "incident_minute_data = df_verify[\n",
    "    (df_verify['datetime'] >= incident_start) &\n",
    "    (df_verify['datetime'] < incident_start + timedelta(minutes=1))\n",
    "]\n",
    "\n",
    "print(f\"  Data at incident time (22:10): {len(incident_minute_data)} transactions\")\n",
    "if len(incident_minute_data) > 0:\n",
    "    print(f\"  Avg performance at 22:10: {incident_minute_data['execution_time'].mean():.2f}s\")\n",
    "\n",
    "# Check for data gaps\n",
    "print(f\"\\nüîé DATA CONTINUITY CHECK:\")\n",
    "hourly_counts = df_verify.groupby(df_verify['datetime'].dt.floor('H')).size()\n",
    "print(f\"Hourly transaction counts:\")\n",
    "for hour, count in hourly_counts.items():\n",
    "    print(f\"  {hour.strftime('%Y-%m-%d %H:00')}: {count:,} transactions\")\n",
    "\n",
    "# Identify any significant gaps\n",
    "gaps = hourly_counts[hourly_counts < 100]  # Flag hours with <100 transactions\n",
    "if len(gaps) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è POTENTIAL DATA GAPS (hours with <100 transactions):\")\n",
    "    for hour, count in gaps.items():\n",
    "        print(f\"  {hour.strftime('%Y-%m-%d %H:00')}: {count} transactions\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No significant data gaps detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evidence Summary & Accuracy Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã EVIDENCE VERIFICATION SUMMARY REPORT\n",
      "============================================================\n",
      "Verification completed: 2025-11-08 01:04:02\n",
      "Data verified: 158,186 total transactions\n",
      "\n",
      "‚úÖ VERIFIED FINDINGS:\n",
      "  1. Baseline performance: 2.03s average response time\n",
      "  2. Peak impact performance: 13.19s average response time\n",
      "  3. Performance degradation: 551% (6.5x slower)\n",
      "  4. Baseline transaction count: 2,756\n",
      "  5. Peak period transaction count: 4,708\n",
      "  6. Worst single transaction: 119.2s at 14:11:30\n",
      "  7. Total critical transactions (>60s): 211\n",
      "  8. Total slow transactions (>20s): 3,436\n",
      "  9. Analysis timeframe: 2025-11-06 00:00:00.850000 to 2025-11-07 03:33:49.509000\n",
      "\n",
      "üìä DATA QUALITY ASSESSMENT:\n",
      "  Total records processed: 158,186\n",
      "  Time6.txt records: 141,657\n",
      "  Time.txt records: 16,529\n",
      "  Date range coverage: 27.6 hours\n",
      "  ‚úÖ No data quality issues detected\n",
      "\n",
      "üéØ RECOMMENDATIONS FOR FINAL REPORT:\n",
      "  1. Use only verified statistics from this analysis\n",
      "  2. Include specific timestamps and transaction IDs as evidence\n",
      "  3. Reference source files and line numbers for traceability\n",
      "  4. Avoid extrapolation beyond what data supports\n",
      "  5. Clearly distinguish between calculated metrics and raw observations\n",
      "\n",
      "============================================================\n",
      "‚úÖ Evidence verification complete - Ready for final report\n"
     ]
    }
   ],
   "source": [
    "# Generate final evidence-based summary\n",
    "print(\"üìã EVIDENCE VERIFICATION SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Verification completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Data verified: {len(df_verify):,} total transactions\")\n",
    "\n",
    "print(f\"\\n‚úÖ VERIFIED FINDINGS:\")\n",
    "\n",
    "# Only include findings we can verify from the data\n",
    "verified_facts = []\n",
    "\n",
    "if 'period_stats' in locals() and len(period_stats) > 0:\n",
    "    # Find actual baseline and peak\n",
    "    pre_incident = period_stats.get('Pre-incident (21:30-22:00)', {})\n",
    "    peak_impact = period_stats.get('Peak Impact (22:20-23:10)', {})\n",
    "    \n",
    "    if pre_incident and peak_impact:\n",
    "        baseline_mean = pre_incident['mean']\n",
    "        peak_mean = peak_impact['mean']\n",
    "        degradation_factor = peak_mean / baseline_mean\n",
    "        degradation_pct = ((peak_mean - baseline_mean) / baseline_mean) * 100\n",
    "        \n",
    "        verified_facts.extend([\n",
    "            f\"Baseline performance: {baseline_mean:.2f}s average response time\",\n",
    "            f\"Peak impact performance: {peak_mean:.2f}s average response time\",\n",
    "            f\"Performance degradation: {degradation_pct:.0f}% ({degradation_factor:.1f}x slower)\",\n",
    "            f\"Baseline transaction count: {pre_incident['count']:,}\",\n",
    "            f\"Peak period transaction count: {peak_impact['count']:,}\"\n",
    "        ])\n",
    "\n",
    "# Add verified transaction data\n",
    "if len(df_verify) > 0:\n",
    "    worst_transaction = df_verify.loc[df_verify['execution_time'].idxmax()]\n",
    "    total_critical = (df_verify['execution_time'] > 60).sum()\n",
    "    total_slow = (df_verify['execution_time'] > 20).sum()\n",
    "    \n",
    "    verified_facts.extend([\n",
    "        f\"Worst single transaction: {worst_transaction['execution_time']:.1f}s at {worst_transaction['datetime'].strftime('%H:%M:%S')}\",\n",
    "        f\"Total critical transactions (>60s): {total_critical:,}\",\n",
    "        f\"Total slow transactions (>20s): {total_slow:,}\",\n",
    "        f\"Analysis timeframe: {df_verify['datetime'].min()} to {df_verify['datetime'].max()}\"\n",
    "    ])\n",
    "\n",
    "# Print verified facts\n",
    "for i, fact in enumerate(verified_facts, 1):\n",
    "    print(f\"  {i}. {fact}\")\n",
    "\n",
    "print(f\"\\nüìä DATA QUALITY ASSESSMENT:\")\n",
    "print(f\"  Total records processed: {len(df_verify):,}\")\n",
    "print(f\"  Time6.txt records: {len(df_time6_verify):,}\")\n",
    "print(f\"  Time.txt records: {len(df_time7_verify):,}\")\n",
    "print(f\"  Date range coverage: {(df_verify['datetime'].max() - df_verify['datetime'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "\n",
    "# Check for any parsing issues\n",
    "parsing_issues = df_verify[df_verify['execution_time'] < 0]  # Impossible values\n",
    "if len(parsing_issues) > 0:\n",
    "    print(f\"  ‚ö†Ô∏è Data quality issues: {len(parsing_issues)} records with invalid values\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ No data quality issues detected\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATIONS FOR FINAL REPORT:\")\n",
    "print(f\"  1. Use only verified statistics from this analysis\")\n",
    "print(f\"  2. Include specific timestamps and transaction IDs as evidence\")\n",
    "print(f\"  3. Reference source files and line numbers for traceability\")\n",
    "print(f\"  4. Avoid extrapolation beyond what data supports\")\n",
    "print(f\"  5. Clearly distinguish between calculated metrics and raw observations\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ Evidence verification complete - Ready for final report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
