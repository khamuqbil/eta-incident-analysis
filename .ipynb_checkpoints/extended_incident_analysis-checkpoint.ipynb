{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended ETA Agent Incident Analysis - 22:10 to 04:39\n",
    "\n",
    "**Extended Analysis Scope:**\n",
    "- **Reported Issue**: Slowness and timeout observed at 19:10 UTC (6 Nov 2025)\n",
    "- **Local Time**: 22:10 UTC+3 (6 Nov 2025)\n",
    "- **Analysis Period**: 21:30 (6 Nov) ‚Üí 03:30 (7 Nov)\n",
    "- **Focus Window**: 22:10 ‚Üí 04:39 (extended incident tracking)\n",
    "\n",
    "## Extended Analysis Objectives\n",
    "1. **Complete incident timeline** - from start to full recovery\n",
    "2. **Extended impact assessment** - how long did issues persist?\n",
    "3. **Recovery pattern analysis** - gradual vs sudden recovery\n",
    "4. **Night-shift impact** - did issues affect overnight operations?\n",
    "5. **Performance normalization** - when did system fully stabilize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set3\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"üìä Extended Incident Analysis Environment Ready!\")\n",
    "print(f\"üïê Incident Start: 22:10 UTC+3 (6 Nov 2025)\")\n",
    "print(f\"üîç Focus Window: 22:10 ‚Üí 04:39 (6+ hour analysis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading - Extended Timeline Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_extended_logs(filepath, start_time, end_time, is_time6=True):\n",
    "    \"\"\"\n",
    "    Parse logs for extended incident analysis\n",
    "    start_time and end_time should be datetime objects\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    incident_time = datetime(2025, 11, 6, 22, 10)  # 22:10 on 6 Nov\n",
    "    focus_end = datetime(2025, 11, 7, 4, 39)       # 04:39 on 7 Nov\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Extract components\n",
    "            pattern = r'(eta_\\w+)\\.(\\d+):([\\d/]+) ([\\d:.]+) (\\d+) TOOK ([\\d.]+)s'\n",
    "            match = re.match(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                agent_type, pid, date, time, transaction_id, execution_time = match.groups()\n",
    "                \n",
    "                # Parse datetime\n",
    "                dt = datetime.strptime(f\"{date} {time}\", \"%d/%m/%Y %H:%M:%S.%f\")\n",
    "                \n",
    "                # Filter for our extended timeframe\n",
    "                if start_time <= dt <= end_time:\n",
    "                    # Calculate time relative to incident (in minutes)\n",
    "                    time_to_incident = (dt - incident_time).total_seconds() / 60\n",
    "                    \n",
    "                    # Extended period classification\n",
    "                    if time_to_incident < -40:  # Before 21:30\n",
    "                        period = \"Pre-incident (Early)\"\n",
    "                    elif time_to_incident < -10:  # 21:30 to 22:00\n",
    "                        period = \"Pre-incident (Late)\"\n",
    "                    elif time_to_incident < 10:   # 22:00 to 22:20\n",
    "                        period = \"Incident Start\"\n",
    "                    elif time_to_incident < 60:   # 22:20 to 23:10\n",
    "                        period = \"Peak Impact\"\n",
    "                    elif time_to_incident < 120:  # 23:10 to 00:10\n",
    "                        period = \"Initial Recovery\"\n",
    "                    elif time_to_incident < 240:  # 00:10 to 02:10\n",
    "                        period = \"Mid Recovery\"\n",
    "                    else:  # 02:10 onwards\n",
    "                        period = \"Late Recovery\"\n",
    "                    \n",
    "                    # Additional classifications\n",
    "                    in_focus_window = incident_time <= dt <= focus_end\n",
    "                    hour_of_day = dt.hour\n",
    "                    \n",
    "                    # Business hours classification\n",
    "                    if 8 <= hour_of_day <= 17:\n",
    "                        shift = \"Business Hours\"\n",
    "                    elif 18 <= hour_of_day <= 23:\n",
    "                        shift = \"Evening Shift\"\n",
    "                    else:\n",
    "                        shift = \"Night Shift\"\n",
    "                    \n",
    "                    data.append({\n",
    "                        'agent_type': agent_type,\n",
    "                        'pid': int(pid),\n",
    "                        'datetime': dt,\n",
    "                        'date': dt.date(),\n",
    "                        'hour': dt.hour,\n",
    "                        'minute': dt.minute,\n",
    "                        'transaction_id': int(transaction_id),\n",
    "                        'execution_time': float(execution_time),\n",
    "                        'time_to_incident_min': time_to_incident,\n",
    "                        'period': period,\n",
    "                        'shift': shift,\n",
    "                        'in_focus_window': in_focus_window,\n",
    "                        'is_slow': float(execution_time) > 20,\n",
    "                        'is_very_slow': float(execution_time) > 30,\n",
    "                        'is_critical': float(execution_time) > 60,  # New threshold\n",
    "                        'source_file': 'time6.txt' if is_time6 else 'time.txt'\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"Extended log parser ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extended time ranges\n",
    "analysis_start = datetime(2025, 11, 6, 21, 30)  # 21:30 on 6 Nov\n",
    "analysis_end = datetime(2025, 11, 7, 3, 30)     # 03:30 on 7 Nov\n",
    "focus_start = datetime(2025, 11, 6, 22, 10)     # 22:10 incident time\n",
    "focus_end = datetime(2025, 11, 7, 4, 39)        # 04:39 end focus\n",
    "\n",
    "print(f\"üìä Loading extended dataset...\")\n",
    "print(f\"‚è∞ Analysis Period: {analysis_start} ‚Üí {analysis_end}\")\n",
    "print(f\"üîç Focus Window: {focus_start} ‚Üí {focus_end}\")\n",
    "\n",
    "# Load from time6.txt (6 Nov data)\n",
    "df_time6 = parse_extended_logs('time6.txt', analysis_start, \n",
    "                              datetime(2025, 11, 6, 23, 59, 59), is_time6=True)\n",
    "\n",
    "# Load from time.txt (7 Nov data)\n",
    "df_time7 = parse_extended_logs('time.txt', \n",
    "                              datetime(2025, 11, 7, 0, 0, 0), analysis_end, is_time6=False)\n",
    "\n",
    "# Combine datasets\n",
    "df_extended = pd.concat([df_time6, df_time7], ignore_index=True)\n",
    "df_extended = df_extended.sort_values('datetime')\n",
    "\n",
    "print(f\"üìà Loaded {len(df_time6)} transactions from 6 Nov\")\n",
    "print(f\"üìà Loaded {len(df_time7)} transactions from 7 Nov\")\n",
    "print(f\"üìä Total extended dataset: {len(df_extended):,} transactions\")\n",
    "print(f\"‚è∞ Time range: {df_extended['datetime'].min()} to {df_extended['datetime'].max()}\")\n",
    "\n",
    "# Focus window subset\n",
    "df_focus = df_extended[df_extended['in_focus_window']].copy()\n",
    "print(f\"üîç Focus window contains: {len(df_focus):,} transactions\")\n",
    "\n",
    "# Show period distribution\n",
    "print(\"\\nüìã Extended Period Distribution:\")\n",
    "period_counts = df_extended['period'].value_counts()\n",
    "for period, count in period_counts.items():\n",
    "    print(f\"  {period}: {count:,} transactions\")\n",
    "\n",
    "print(\"\\nüïê Shift Distribution:\")\n",
    "shift_counts = df_extended['shift'].value_counts()\n",
    "for shift, count in shift_counts.items():\n",
    "    print(f\"  {shift}: {count:,} transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extended Incident Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive performance metrics by period\n",
    "extended_stats = df_extended.groupby('period')['execution_time'].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max',\n",
    "    lambda x: x.quantile(0.95),\n",
    "    lambda x: x.quantile(0.99),\n",
    "    lambda x: (x > 20).sum(),   # slow\n",
    "    lambda x: (x > 30).sum(),   # very slow\n",
    "    lambda x: (x > 60).sum(),   # critical\n",
    "    lambda x: (x > 20).sum() / len(x) * 100,  # slow %\n",
    "    lambda x: (x > 60).sum() / len(x) * 100   # critical %\n",
    "]).round(3)\n",
    "\n",
    "extended_stats.columns = ['Count', 'Mean', 'Median', 'Std', 'Min', 'Max', \n",
    "                         'P95', 'P99', 'Slow_Count', 'Very_Slow_Count', \n",
    "                         'Critical_Count', 'Slow_Percent', 'Critical_Percent']\n",
    "\n",
    "print(\"üö® EXTENDED INCIDENT IMPACT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(extended_stats)\n",
    "\n",
    "# Calculate degradation from baseline (Pre-incident Early)\n",
    "if 'Pre-incident (Early)' in extended_stats.index:\n",
    "    baseline = extended_stats.loc['Pre-incident (Early)']\n",
    "    baseline_mean = baseline['Mean']\n",
    "    baseline_slow_pct = baseline['Slow_Percent']\n",
    "    \n",
    "    print(f\"\\nüìä PERFORMANCE DEGRADATION FROM BASELINE:\")\n",
    "    print(f\"  Baseline (Pre-incident Early): {baseline_mean:.2f}s avg, {baseline_slow_pct:.1f}% slow\")\n",
    "    print(\"\\nüîç Degradation by Period:\")\n",
    "    \n",
    "    for period in extended_stats.index:\n",
    "        if period != 'Pre-incident (Early)':\n",
    "            period_mean = extended_stats.loc[period, 'Mean']\n",
    "            period_slow_pct = extended_stats.loc[period, 'Slow_Percent']\n",
    "            degradation = ((period_mean - baseline_mean) / baseline_mean) * 100\n",
    "            slow_increase = period_slow_pct - baseline_slow_pct\n",
    "            \n",
    "            print(f\"  {period:.<20} {degradation:+6.1f}% performance, {slow_increase:+5.1f}% slow txns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Extended Metrics Explanation\n",
    "\n",
    "**Extended Performance Periods:**\n",
    "- **Pre-incident (Early)**: Normal baseline before any issues (21:30-21:30)\n",
    "- **Pre-incident (Late)**: Just before incident (21:30-22:00)\n",
    "- **Incident Start**: Initial impact window (22:00-22:20)\n",
    "- **Peak Impact**: Maximum degradation period (22:20-23:10)\n",
    "- **Initial Recovery**: First recovery phase (23:10-00:10)\n",
    "- **Mid Recovery**: Continued improvement (00:10-02:10)\n",
    "- **Late Recovery**: Final stabilization (02:10+)\n",
    "\n",
    "**Additional Severity Thresholds:**\n",
    "- **Critical_Count/Percent**: Transactions >60 seconds (severe user impact)\n",
    "- **Performance degradation %**: How much slower compared to baseline\n",
    "- **Slow transaction increase**: Additional % of users affected\n",
    "\n",
    "**üí° Extended Analysis Benefits:**\n",
    "- **Complete incident lifecycle** - from start to full recovery\n",
    "- **Recovery pattern identification** - gradual vs sudden improvement\n",
    "- **Night shift impact assessment** - operational continuity\n",
    "- **True recovery time** - when system fully stabilized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extended Timeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive extended timeline visualization\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 16))\n",
    "\n",
    "# 1. Complete performance timeline\n",
    "ax1 = axes[0]\n",
    "df_plot = df_extended.sort_values('datetime')\n",
    "\n",
    "# Plot all transactions with color coding\n",
    "normal_mask = ~df_plot['is_slow']\n",
    "slow_mask = df_plot['is_slow'] & ~df_plot['is_very_slow']\n",
    "very_slow_mask = df_plot['is_very_slow'] & ~df_plot['is_critical']\n",
    "critical_mask = df_plot['is_critical']\n",
    "\n",
    "ax1.scatter(df_plot[normal_mask]['datetime'], df_plot[normal_mask]['execution_time'], \n",
    "           alpha=0.4, s=4, c='blue', label='Normal (<20s)')\n",
    "ax1.scatter(df_plot[slow_mask]['datetime'], df_plot[slow_mask]['execution_time'], \n",
    "           alpha=0.7, s=8, c='orange', label='Slow (20-30s)')\n",
    "ax1.scatter(df_plot[very_slow_mask]['datetime'], df_plot[very_slow_mask]['execution_time'], \n",
    "           alpha=0.8, s=12, c='red', label='Very Slow (30-60s)')\n",
    "ax1.scatter(df_plot[critical_mask]['datetime'], df_plot[critical_mask]['execution_time'], \n",
    "           alpha=1.0, s=16, c='darkred', label='Critical (>60s)')\n",
    "\n",
    "# Mark key times\n",
    "incident_time = datetime(2025, 11, 6, 22, 10)\n",
    "focus_end_time = datetime(2025, 11, 7, 4, 39)\n",
    "ax1.axvline(x=incident_time, color='red', linestyle='--', linewidth=2, label='Incident Start (22:10)')\n",
    "ax1.axvline(x=focus_end_time, color='green', linestyle='--', linewidth=2, label='Focus End (04:39)')\n",
    "\n",
    "ax1.set_ylabel('Execution Time (seconds)')\n",
    "ax1.set_title('Extended ETA Agent Performance Timeline (6+ Hours)')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. 10-minute rolling averages for trend analysis\n",
    "ax2 = axes[1]\n",
    "df_plot['datetime_10min'] = df_plot['datetime'].dt.floor('10T')\n",
    "rolling_10min = df_plot.groupby('datetime_10min')['execution_time'].agg(\n",
    "    ['mean', 'count', 'std', lambda x: (x > 20).sum() / len(x) * 100]\n",
    ").reset_index()\n",
    "rolling_10min.columns = ['datetime', 'mean', 'count', 'std', 'slow_pct']\n",
    "\n",
    "# Plot rolling average with error bands\n",
    "ax2.plot(rolling_10min['datetime'], rolling_10min['mean'], linewidth=2, color='blue', label='10-min avg')\n",
    "ax2.fill_between(rolling_10min['datetime'], \n",
    "                rolling_10min['mean'] - rolling_10min['std'], \n",
    "                rolling_10min['mean'] + rolling_10min['std'], \n",
    "                alpha=0.3, color='blue', label='¬±1 std dev')\n",
    "\n",
    "ax2.axvline(x=incident_time, color='red', linestyle='--', linewidth=2)\n",
    "ax2.axvline(x=focus_end_time, color='green', linestyle='--', linewidth=2)\n",
    "ax2.set_ylabel('Avg Execution Time (s)')\n",
    "ax2.set_title('10-Minute Rolling Average Performance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Slow transaction percentage over time\n",
    "ax3 = axes[2]\n",
    "ax3.plot(rolling_10min['datetime'], rolling_10min['slow_pct'], \n",
    "         linewidth=2, color='red', marker='o', markersize=3)\n",
    "ax3.axvline(x=incident_time, color='red', linestyle='--', linewidth=2)\n",
    "ax3.axvline(x=focus_end_time, color='green', linestyle='--', linewidth=2)\n",
    "ax3.set_ylabel('Slow Transactions (%)')\n",
    "ax3.set_title('Slow Transaction Rate Over Time')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Transaction volume and system load\n",
    "ax4 = axes[3]\n",
    "ax4.bar(rolling_10min['datetime'], rolling_10min['count'], \n",
    "        width=timedelta(minutes=8), alpha=0.7, color='green')\n",
    "ax4.axvline(x=incident_time, color='red', linestyle='--', linewidth=2)\n",
    "ax4.axvline(x=focus_end_time, color='green', linestyle='--', linewidth=2)\n",
    "ax4.set_ylabel('Transactions per 10min')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_title('System Load (Transaction Volume)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key timeline observations\n",
    "peak_slow_time = rolling_10min.loc[rolling_10min['slow_pct'].idxmax(), 'datetime']\n",
    "peak_slow_pct = rolling_10min['slow_pct'].max()\n",
    "min_volume_time = rolling_10min.loc[rolling_10min['count'].idxmin(), 'datetime']\n",
    "min_volume = rolling_10min['count'].min()\n",
    "\n",
    "print(f\"üìä EXTENDED TIMELINE KEY OBSERVATIONS:\")\n",
    "print(f\"üö® Peak slow transaction rate: {peak_slow_pct:.1f}% at {peak_slow_time.strftime('%H:%M')}\")\n",
    "print(f\"üìâ Minimum transaction volume: {min_volume} at {min_volume_time.strftime('%H:%M')}\")\n",
    "print(f\"‚è±Ô∏è Total analysis duration: {(df_extended['datetime'].max() - df_extended['datetime'].min()).total_seconds() / 3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Focus Window Deep Dive (22:10 ‚Üí 04:39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep analysis of the 6+ hour focus window\n",
    "print(f\"üîç FOCUS WINDOW DEEP DIVE (22:10 ‚Üí 04:39)\")\n",
    "print(f\"üìä Focus window transactions: {len(df_focus):,}\")\n",
    "print(f\"‚è±Ô∏è Focus duration: {(focus_end - focus_start).total_seconds() / 3600:.1f} hours\")\n",
    "\n",
    "# Hourly breakdown within focus window\n",
    "df_focus['hour_mark'] = df_focus['datetime'].dt.floor('H')\n",
    "hourly_focus = df_focus.groupby('hour_mark')['execution_time'].agg([\n",
    "    'count', 'mean', 'median', 'max',\n",
    "    lambda x: x.quantile(0.95),\n",
    "    lambda x: (x > 20).sum(),\n",
    "    lambda x: (x > 30).sum(),\n",
    "    lambda x: (x > 60).sum(),\n",
    "    lambda x: (x > 20).sum() / len(x) * 100\n",
    "]).round(2)\n",
    "\n",
    "hourly_focus.columns = ['Count', 'Mean', 'Median', 'Max', 'P95', \n",
    "                       'Slow', 'Very_Slow', 'Critical', 'Slow_Pct']\n",
    "\n",
    "print(\"\\n‚è∞ HOURLY BREAKDOWN IN FOCUS WINDOW:\")\n",
    "print(hourly_focus)\n",
    "\n",
    "# Identify worst performing hours\n",
    "worst_hour_mean = hourly_focus['Mean'].idxmax()\n",
    "worst_hour_slow = hourly_focus['Slow_Pct'].idxmax()\n",
    "best_hour_mean = hourly_focus['Mean'].idxmin()\n",
    "\n",
    "print(f\"\\nüö® FOCUS WINDOW INSIGHTS:\")\n",
    "print(f\"  Worst avg performance: {worst_hour_mean.strftime('%H:00')} ({hourly_focus.loc[worst_hour_mean, 'Mean']:.2f}s)\")\n",
    "print(f\"  Highest slow rate: {worst_hour_slow.strftime('%H:00')} ({hourly_focus.loc[worst_hour_slow, 'Slow_Pct']:.1f}%)\")\n",
    "print(f\"  Best performance: {best_hour_mean.strftime('%H:00')} ({hourly_focus.loc[best_hour_mean, 'Mean']:.2f}s)\")\n",
    "\n",
    "# Critical transaction analysis\n",
    "critical_transactions = df_focus[df_focus['is_critical']].copy()\n",
    "if len(critical_transactions) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è CRITICAL TRANSACTIONS (>60s) IN FOCUS WINDOW:\")\n",
    "    print(f\"  Total critical transactions: {len(critical_transactions)}\")\n",
    "    print(f\"  Critical transaction rate: {len(critical_transactions)/len(df_focus)*100:.1f}%\")\n",
    "    \n",
    "    # Show worst critical transactions\n",
    "    worst_critical = critical_transactions.nlargest(5, 'execution_time')[[\n",
    "        'datetime', 'pid', 'execution_time', 'transaction_id'\n",
    "    ]]\n",
    "    print(\"\\n  Top 5 worst critical transactions:\")\n",
    "    for idx, row in worst_critical.iterrows():\n",
    "        print(f\"    {row['datetime'].strftime('%H:%M:%S')} | PID {row['pid']} | {row['execution_time']:.1f}s | TXN {row['transaction_id']}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No critical transactions (>60s) found in focus window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recovery Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recovery patterns in detail\n",
    "recovery_periods = ['Initial Recovery', 'Mid Recovery', 'Late Recovery']\n",
    "recovery_data = df_extended[df_extended['period'].isin(recovery_periods)]\n",
    "\n",
    "if len(recovery_data) > 0:\n",
    "    print(\"üîÑ DETAILED RECOVERY PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 15-minute windows for detailed recovery tracking\n",
    "    recovery_data_sorted = recovery_data.sort_values('datetime')\n",
    "    recovery_data_sorted['window_15min'] = recovery_data_sorted['datetime'].dt.floor('15T')\n",
    "    \n",
    "    recovery_windows = recovery_data_sorted.groupby('window_15min')['execution_time'].agg([\n",
    "        'count', 'mean', 'std',\n",
    "        lambda x: (x > 20).sum() / len(x) * 100,\n",
    "        lambda x: x.quantile(0.95)\n",
    "    ]).round(2)\n",
    "    \n",
    "    recovery_windows.columns = ['Count', 'Mean', 'Std', 'Slow_Pct', 'P95']\n",
    "    \n",
    "    print(\"15-minute recovery windows:\")\n",
    "    print(recovery_windows)\n",
    "    \n",
    "    # Calculate recovery metrics\n",
    "    if 'Pre-incident (Early)' in extended_stats.index:\n",
    "        baseline_mean = extended_stats.loc['Pre-incident (Early)', 'Mean']\n",
    "        baseline_slow_pct = extended_stats.loc['Pre-incident (Early)', 'Slow_Percent']\n",
    "        recovery_threshold = baseline_mean * 1.5  # 50% tolerance\n",
    "        \n",
    "        # Find recovery milestones\n",
    "        recovered_windows = recovery_windows[\n",
    "            (recovery_windows['Mean'] <= recovery_threshold) &\n",
    "            (recovery_windows['Slow_Pct'] <= baseline_slow_pct * 2)\n",
    "        ]\n",
    "        \n",
    "        if len(recovered_windows) > 0:\n",
    "            first_recovery = recovered_windows.index[0]\n",
    "            incident_time = datetime(2025, 11, 6, 22, 10)\n",
    "            recovery_duration = (first_recovery - incident_time).total_seconds() / 60\n",
    "            \n",
    "            print(f\"\\n‚úÖ RECOVERY MILESTONES:\")\n",
    "            print(f\"  Incident start: {incident_time.strftime('%H:%M')}\")\n",
    "            print(f\"  Performance threshold met: {first_recovery.strftime('%H:%M')}\")\n",
    "            print(f\"  Time to recovery: {recovery_duration:.0f} minutes ({recovery_duration/60:.1f} hours)\")\n",
    "            print(f\"  Recovery threshold: {recovery_threshold:.2f}s (vs baseline {baseline_mean:.2f}s)\")\n",
    "        \n",
    "        # Recovery rate analysis\n",
    "        recovery_trend = recovery_windows['Mean'].diff().dropna()\n",
    "        improving_windows = (recovery_trend < 0).sum()\n",
    "        degrading_windows = (recovery_trend > 0).sum()\n",
    "        \n",
    "        print(f\"\\nüìà RECOVERY TREND ANALYSIS:\")\n",
    "        print(f\"  Improving windows: {improving_windows}\")\n",
    "        print(f\"  Degrading windows: {degrading_windows}\")\n",
    "        print(f\"  Recovery consistency: {improving_windows/(improving_windows+degrading_windows)*100:.1f}%\")\n",
    "    \n",
    "    # Visualize recovery pattern\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Recovery performance trend\n",
    "    ax1.plot(recovery_windows.index, recovery_windows['Mean'], 'o-', \n",
    "            linewidth=2, markersize=6, color='blue', label='Mean performance')\n",
    "    ax1.plot(recovery_windows.index, recovery_windows['P95'], 'o-', \n",
    "            linewidth=2, markersize=4, color='orange', label='P95 performance')\n",
    "    \n",
    "    if 'Pre-incident (Early)' in extended_stats.index:\n",
    "        ax1.axhline(y=baseline_mean, color='green', linestyle='--', \n",
    "                   label=f'Baseline ({baseline_mean:.2f}s)')\n",
    "        ax1.axhline(y=recovery_threshold, color='orange', linestyle='--', \n",
    "                   label=f'Recovery threshold ({recovery_threshold:.2f}s)')\n",
    "    \n",
    "    ax1.set_ylabel('Execution Time (seconds)')\n",
    "    ax1.set_title('Performance Recovery Timeline (15-min windows)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Recovery slow transaction trend\n",
    "    ax2.plot(recovery_windows.index, recovery_windows['Slow_Pct'], 'o-', \n",
    "            linewidth=2, markersize=6, color='red')\n",
    "    if 'Pre-incident (Early)' in extended_stats.index:\n",
    "        ax2.axhline(y=baseline_slow_pct, color='green', linestyle='--',\n",
    "                   label=f'Baseline ({baseline_slow_pct:.1f}%)')\n",
    "    ax2.set_ylabel('Slow Transactions (%)')\n",
    "    ax2.set_xlabel('Time Window')\n",
    "    ax2.set_title('Slow Transaction Recovery')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No recovery period data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Night Shift Impact Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact on different operational shifts\n",
    "print(\"üåô NIGHT SHIFT IMPACT ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Performance by shift\n",
    "shift_stats = df_extended.groupby('shift')['execution_time'].agg([\n",
    "    'count', 'mean', 'median', 'std',\n",
    "    lambda x: x.quantile(0.95),\n",
    "    lambda x: (x > 20).sum() / len(x) * 100,\n",
    "    lambda x: (x > 30).sum() / len(x) * 100\n",
    "]).round(2)\n",
    "\n",
    "shift_stats.columns = ['Count', 'Mean', 'Median', 'Std', 'P95', 'Slow_Pct', 'Very_Slow_Pct']\n",
    "\n",
    "print(\"Performance by operational shift:\")\n",
    "print(shift_stats)\n",
    "\n",
    "# Night shift detailed analysis\n",
    "night_data = df_extended[df_extended['shift'] == 'Night Shift']\n",
    "evening_data = df_extended[df_extended['shift'] == 'Evening Shift']\n",
    "\n",
    "if len(night_data) > 0 and len(evening_data) > 0:\n",
    "    night_mean = shift_stats.loc['Night Shift', 'Mean']\n",
    "    evening_mean = shift_stats.loc['Evening Shift', 'Mean']\n",
    "    night_slow_pct = shift_stats.loc['Night Shift', 'Slow_Pct']\n",
    "    evening_slow_pct = shift_stats.loc['Evening Shift', 'Slow_Pct']\n",
    "    \n",
    "    print(f\"\\nüîç SHIFT COMPARISON:\")\n",
    "    print(f\"  Evening shift performance: {evening_mean:.2f}s avg, {evening_slow_pct:.1f}% slow\")\n",
    "    print(f\"  Night shift performance: {night_mean:.2f}s avg, {night_slow_pct:.1f}% slow\")\n",
    "    \n",
    "    night_impact = ((night_mean - evening_mean) / evening_mean) * 100\n",
    "    print(f\"  Night shift impact: {night_impact:+.1f}% compared to evening\")\n",
    "    \n",
    "    # Night shift hourly breakdown\n",
    "    night_hourly = night_data.groupby('hour')['execution_time'].agg([\n",
    "        'count', 'mean', \n",
    "        lambda x: (x > 20).sum() / len(x) * 100 if len(x) > 0 else 0\n",
    "    ]).round(2)\n",
    "    night_hourly.columns = ['Count', 'Mean', 'Slow_Pct']\n",
    "    \n",
    "    print(f\"\\nüåô NIGHT SHIFT HOURLY BREAKDOWN (00:00-07:59):\")\n",
    "    print(night_hourly)\n",
    "    \n",
    "    # Visualize shift performance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Shift performance comparison\n",
    "    shifts = shift_stats.index\n",
    "    ax1.bar(shifts, shift_stats['Mean'], alpha=0.7, color=['orange', 'blue', 'purple'][:len(shifts)])\n",
    "    ax1.set_ylabel('Average Execution Time (s)')\n",
    "    ax1.set_title('Performance by Operational Shift')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Night shift hourly trend\n",
    "    if len(night_hourly) > 0:\n",
    "        ax2.plot(night_hourly.index, night_hourly['Mean'], 'o-', \n",
    "                linewidth=2, markersize=6, color='darkblue')\n",
    "        ax2.set_xlabel('Hour of Day')\n",
    "        ax2.set_ylabel('Average Execution Time (s)')\n",
    "        ax2.set_title('Night Shift Performance by Hour')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xticks(range(0, 24, 2))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Operational recommendations\n",
    "    print(f\"\\nüí° OPERATIONAL IMPACT:\")\n",
    "    if night_impact > 20:\n",
    "        print(f\"  ‚ö†Ô∏è Night shift significantly affected - consider additional monitoring\")\n",
    "    elif night_impact > 0:\n",
    "        print(f\"  üìä Night shift moderately impacted - review overnight procedures\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Night shift performance maintained well\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient data for comprehensive shift analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extended Summary Report & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive extended incident report\n",
    "print(\"üìã EXTENDED INCIDENT ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Extended Analysis Period: {df_extended['datetime'].min()} to {df_extended['datetime'].max()}\")\n",
    "print(f\"Focus Window: 22:10 (6 Nov) ‚Üí 04:39 (7 Nov)\")\n",
    "print(f\"Total Transactions Analyzed: {len(df_extended):,}\")\n",
    "print(f\"Focus Window Transactions: {len(df_focus):,}\")\n",
    "analysis_duration = (df_extended['datetime'].max() - df_extended['datetime'].min()).total_seconds() / 3600\n",
    "print(f\"Analysis Duration: {analysis_duration:.1f} hours\")\n",
    "\n",
    "# Extended key findings\n",
    "print(\"\\nüìä EXTENDED KEY FINDINGS:\")\n",
    "\n",
    "if 'Pre-incident (Early)' in extended_stats.index:\n",
    "    baseline = extended_stats.loc['Pre-incident (Early)']\n",
    "    \n",
    "    # Find peak impact\n",
    "    peak_period = extended_stats['Mean'].idxmax()\n",
    "    peak_stats = extended_stats.loc[peak_period]\n",
    "    \n",
    "    baseline_mean = baseline['Mean']\n",
    "    peak_mean = peak_stats['Mean']\n",
    "    peak_degradation = ((peak_mean - baseline_mean) / baseline_mean) * 100\n",
    "    \n",
    "    print(f\"‚Ä¢ Baseline performance: {baseline_mean:.2f}s avg, {baseline['Slow_Percent']:.1f}% slow\")\n",
    "    print(f\"‚Ä¢ Peak impact period: {peak_period} ({peak_mean:.2f}s avg, {peak_stats['Slow_Percent']:.1f}% slow)\")\n",
    "    print(f\"‚Ä¢ Maximum degradation: {peak_degradation:+.0f}% performance loss\")\n",
    "    \n",
    "    # Recovery assessment\n",
    "    final_period = extended_stats.loc['Late Recovery']\n",
    "    final_recovery = ((final_period['Mean'] - baseline_mean) / baseline_mean) * 100\n",
    "    print(f\"‚Ä¢ Final recovery status: {final_recovery:+.1f}% vs baseline\")\n",
    "    \n",
    "    # Critical transaction impact\n",
    "    total_critical = df_extended['is_critical'].sum()\n",
    "    focus_critical = df_focus['is_critical'].sum()\n",
    "    print(f\"‚Ä¢ Critical transactions (>60s): {total_critical} total, {focus_critical} in focus window\")\n",
    "\n",
    "# Extended severity assessment\n",
    "print(\"\\nüö® EXTENDED SEVERITY ASSESSMENT:\")\n",
    "severity_score = 0\n",
    "\n",
    "if peak_degradation > 300: severity_score += 3\n",
    "elif peak_degradation > 200: severity_score += 2\n",
    "elif peak_degradation > 100: severity_score += 1\n",
    "\n",
    "if analysis_duration > 4: severity_score += 2\n",
    "elif analysis_duration > 2: severity_score += 1\n",
    "\n",
    "if total_critical > 100: severity_score += 2\n",
    "elif total_critical > 50: severity_score += 1\n",
    "\n",
    "if severity_score >= 6:\n",
    "    severity = \"CRITICAL - MAJOR INCIDENT\"\n",
    "elif severity_score >= 4:\n",
    "    severity = \"HIGH - SIGNIFICANT INCIDENT\"\n",
    "elif severity_score >= 2:\n",
    "    severity = \"MEDIUM - NOTABLE INCIDENT\"\n",
    "else:\n",
    "    severity = \"LOW - MINOR INCIDENT\"\n",
    "\n",
    "print(f\"‚Ä¢ Extended Severity Classification: {severity}\")\n",
    "print(f\"‚Ä¢ Severity Score: {severity_score}/7\")\n",
    "\n",
    "# Extended operational recommendations\n",
    "print(\"\\nüéØ EXTENDED RECOMMENDATIONS:\")\n",
    "print(\"üìã Immediate Actions:\")\n",
    "print(\"  ‚Ä¢ Implement real-time alerting for >50% performance degradation\")\n",
    "print(\"  ‚Ä¢ Set up automated escalation for incidents lasting >2 hours\")\n",
    "print(\"  ‚Ä¢ Review capacity planning for peak evening hours (22:00-23:00)\")\n",
    "\n",
    "print(\"\\nüìã Monitoring Enhancements:\")\n",
    "print(\"  ‚Ä¢ Deploy P95/P99 monitoring with 5-minute resolution\")\n",
    "print(\"  ‚Ä¢ Create dashboards for shift-based performance tracking\")\n",
    "print(\"  ‚Ä¢ Establish baseline performance metrics by time of day\")\n",
    "\n",
    "print(\"\\nüìã Preventive Measures:\")\n",
    "if 'Night Shift' in shift_stats.index and shift_stats.loc['Night Shift', 'Mean'] > baseline_mean * 2:\n",
    "    print(\"  ‚Ä¢ Review night shift procedures and resource allocation\")\n",
    "print(\"  ‚Ä¢ Implement circuit breakers for transactions >60s\")\n",
    "print(\"  ‚Ä¢ Consider load shedding during extreme degradation periods\")\n",
    "print(\"  ‚Ä¢ Review and optimize processes active during 22:10-23:00 timeframe\")\n",
    "\n",
    "print(\"\\nüìã Follow-up Analysis:\")\n",
    "print(\"  ‚Ä¢ Root cause analysis for transactions exceeding 60s\")\n",
    "print(\"  ‚Ä¢ Database/infrastructure review for 22:10-23:00 period\")\n",
    "print(\"  ‚Ä¢ Capacity assessment for similar load patterns\")\n",
    "print(\"  ‚Ä¢ Review incident response procedures and timeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîç Extended Incident Analysis Complete\")\n",
    "print(f\"üìà Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Analysis covered: {analysis_duration:.1f} hours of system operation\")\n",
    "print(f\"üéØ Next review recommended: Post-remediation validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}