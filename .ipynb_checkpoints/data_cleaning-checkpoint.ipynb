{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Time Ordering\n",
    "\n",
    "**Problem Identified:**\n",
    "- Original log files may not be properly time-ordered\n",
    "- This could affect incident analysis accuracy\n",
    "- Need clean, chronologically sorted dataset\n",
    "\n",
    "**Cleaning Objectives:**\n",
    "1. âœ… Parse and validate all log entries\n",
    "2. âœ… Sort by timestamp chronologically\n",
    "3. âœ… Identify and flag any time gaps or anomalies\n",
    "4. âœ… Create clean, ordered output files\n",
    "5. âœ… Verify data integrity and continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\"ðŸ§¹ Data Cleaning Environment Ready!\")\n",
    "print(\"ðŸ“Š Starting comprehensive data validation and ordering...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raw Data Inspection & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_raw_file(filepath):\n",
    "    \"\"\"\n",
    "    Inspect raw file for time ordering and data quality issues\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ” INSPECTING: {filepath}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    timestamps = []\n",
    "    parse_errors = []\n",
    "    line_count = 0\n",
    "    \n",
    "    # Read and extract timestamps\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line_num, line in enumerate(file, 1):\n",
    "            line_count += 1\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Extract timestamp\n",
    "            pattern = r'eta_\\w+\\.\\d+:([\\d/]+) ([\\d:.]+) \\d+ TOOK [\\d.]+s'\n",
    "            match = re.match(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                date_str, time_str = match.groups()\n",
    "                try:\n",
    "                    dt = datetime.strptime(f\"{date_str} {time_str}\", \"%d/%m/%Y %H:%M:%S.%f\")\n",
    "                    timestamps.append({'line': line_num, 'timestamp': dt, 'raw_line': line})\n",
    "                except ValueError as e:\n",
    "                    parse_errors.append({'line': line_num, 'error': str(e), 'raw_line': line[:100]})\n",
    "            else:\n",
    "                parse_errors.append({'line': line_num, 'error': 'Pattern mismatch', 'raw_line': line[:100]})\n",
    "    \n",
    "    print(f\"ðŸ“Š FILE STATISTICS:\")\n",
    "    print(f\"  Total lines: {line_count:,}\")\n",
    "    print(f\"  Valid timestamps: {len(timestamps):,}\")\n",
    "    print(f\"  Parse errors: {len(parse_errors):,}\")\n",
    "    \n",
    "    if timestamps:\n",
    "        # Convert to DataFrame for analysis\n",
    "        df_temp = pd.DataFrame(timestamps)\n",
    "        \n",
    "        # Check time ordering\n",
    "        is_sorted = df_temp['timestamp'].is_monotonic_increasing\n",
    "        \n",
    "        print(f\"\\nâ° TIME ORDERING:\")\n",
    "        print(f\"  First timestamp: {df_temp['timestamp'].min()}\")\n",
    "        print(f\"  Last timestamp: {df_temp['timestamp'].max()}\")\n",
    "        print(f\"  Duration: {(df_temp['timestamp'].max() - df_temp['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "        print(f\"  Chronologically sorted: {'âœ… YES' if is_sorted else 'âŒ NO'}\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df_temp.duplicated('timestamp').sum()\n",
    "        print(f\"  Duplicate timestamps: {duplicates:,}\")\n",
    "        \n",
    "        # Check for time gaps\n",
    "        df_temp = df_temp.sort_values('timestamp')\n",
    "        time_diffs = df_temp['timestamp'].diff()\n",
    "        large_gaps = time_diffs[time_diffs > timedelta(minutes=5)]\n",
    "        \n",
    "        print(f\"  Large time gaps (>5min): {len(large_gaps):,}\")\n",
    "        \n",
    "        if len(large_gaps) > 0:\n",
    "            print(f\"    Largest gap: {large_gaps.max()}\")\n",
    "        \n",
    "        # Show ordering issues\n",
    "        if not is_sorted:\n",
    "            # Find where ordering breaks\n",
    "            for i in range(1, min(len(df_temp), 1000)):\n",
    "                if df_temp.iloc[i]['timestamp'] < df_temp.iloc[i-1]['timestamp']:\n",
    "                    print(f\"\\nâš ï¸ FIRST ORDERING ISSUE FOUND:\")\n",
    "                    print(f\"    Line {df_temp.iloc[i-1]['line']}: {df_temp.iloc[i-1]['timestamp']}\")\n",
    "                    print(f\"    Line {df_temp.iloc[i]['line']}: {df_temp.iloc[i]['timestamp']}\")\n",
    "                    break\n    \n    # Show parse errors (first few)\n    if parse_errors:\n        print(f\"\\nâŒ PARSE ERRORS (first 5):\")\n        for error in parse_errors[:5]:\n            print(f\"    Line {error['line']}: {error['error']}\")\n            print(f\"      {error['raw_line']}\")\n    \n    return timestamps, parse_errors\n\n# Inspect both files\ntime6_data, time6_errors = inspect_raw_file('time6.txt')\ntime7_data, time7_errors = inspect_raw_file('time.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean Data Parsing & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_logs(filepath, file_label):\n",
    "    \"\"\"\n",
    "    Parse logs with comprehensive validation and cleaning\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ§¹ CLEANING: {filepath} ({file_label})\")\n",
    "    \n",
    "    clean_records = []\n",
    "    validation_issues = []\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        for line_num, line in enumerate(file, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Full pattern matching\n",
    "            pattern = r'(eta_\\w+)\\.(\\d+):([\\d/]+) ([\\d:.]+) (\\d+) TOOK ([\\d.]+)s'\n",
    "            match = re.match(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                agent_type, pid, date_str, time_str, transaction_id, execution_time = match.groups()\n",
    "                \n",
    "                try:\n",
    "                    # Parse and validate all components\n",
    "                    dt = datetime.strptime(f\"{date_str} {time_str}\", \"%d/%m/%Y %H:%M:%S.%f\")\n",
    "                    pid_int = int(pid)\n",
    "                    txn_id = int(transaction_id)\n",
    "                    exec_time = float(execution_time)\n",
    "                    \n",
    "                    # Validation checks\n",
    "                    issues = []\n",
    "                    \n",
    "                    # Check for reasonable values\n",
    "                    if exec_time < 0:\n",
    "                        issues.append(\"Negative execution time\")\n",
    "                    elif exec_time > 300:  # >5 minutes seems suspicious\n",
    "                        issues.append(f\"Extremely long execution time: {exec_time}s\")\n",
    "                    \n",
    "                    if pid_int <= 0:\n",
    "                        issues.append(\"Invalid PID\")\n",
    "                    \n",
    "                    if txn_id <= 0:\n",
    "                        issues.append(\"Invalid transaction ID\")\n",
    "                    \n",
    "                    # Check date reasonableness (should be Nov 6-7, 2025)\n",
    "                    if not (datetime(2025, 11, 6) <= dt <= datetime(2025, 11, 8)):\n",
    "                        issues.append(f\"Date outside expected range: {dt.date()}\")\n",
    "                    \n",
    "                    # Store record with metadata\n",
    "                    record = {\n",
    "                        'source_file': file_label,\n",
    "                        'line_number': line_num,\n",
    "                        'agent_type': agent_type,\n",
    "                        'pid': pid_int,\n",
    "                        'datetime': dt,\n",
    "                        'transaction_id': txn_id,\n",
    "                        'execution_time': exec_time,\n",
    "                        'raw_line': line,\n",
    "                        'validation_issues': issues,\n",
    "                        'is_valid': len(issues) == 0\n",
    "                    }\n",
    "                    \n",
    "                    clean_records.append(record)\n",
    "                    \n",
    "                    if issues:\n",
    "                        validation_issues.extend([{\n",
    "                            'line': line_num, \n",
    "                            'issue': issue, \n",
    "                            'record': record\n",
    "                        } for issue in issues])\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    validation_issues.append({\n",
    "                        'line': line_num,\n",
    "                        'issue': f\"Parse error: {str(e)}\",\n",
    "                        'raw_line': line\n",
    "                    })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(clean_records)\n",
    "    \n",
    "    print(f\"  ðŸ“Š Parsed records: {len(clean_records):,}\")\n",
    "    print(f\"  âœ… Valid records: {df['is_valid'].sum():,}\")\n",
    "    print(f\"  âš ï¸ Records with issues: {(~df['is_valid']).sum():,}\")\n",
    "    print(f\"  âŒ Total validation issues: {len(validation_issues):,}\")\n",
    "    \n",
    "    return df, validation_issues\n",
    "\n# Parse both files with cleaning\ndf_time6_clean, time6_issues = parse_clean_logs('time6.txt', 'time6.txt')\ndf_time7_clean, time7_issues = parse_clean_logs('time.txt', 'time.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine and Sort Chronologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\nprint(\"\\nðŸ”— COMBINING DATASETS\")\nprint(\"=\" * 30)\n\n# Combine all records\ndf_combined = pd.concat([df_time6_clean, df_time7_clean], ignore_index=True)\n\nprint(f\"Combined dataset size: {len(df_combined):,} records\")\nprint(f\"Time6.txt: {len(df_time6_clean):,} records\")\nprint(f\"Time.txt: {len(df_time7_clean):,} records\")\n\n# Sort chronologically\nprint(f\"\\nâ° SORTING CHRONOLOGICALLY...\")\ndf_sorted = df_combined.sort_values('datetime').reset_index(drop=True)\n\n# Verify sorting\nprint(f\"âœ… Chronological sorting completed\")\nprint(f\"  First record: {df_sorted['datetime'].iloc[0]}\")\nprint(f\"  Last record: {df_sorted['datetime'].iloc[-1]}\")\nprint(f\"  Total duration: {(df_sorted['datetime'].iloc[-1] - df_sorted['datetime'].iloc[0]).total_seconds() / 3600:.1f} hours\")\n\n# Check for overlaps between files\nif len(df_time6_clean) > 0 and len(df_time7_clean) > 0:\n    time6_max = df_time6_clean['datetime'].max()\n    time7_min = df_time7_clean['datetime'].min()\n    \n    print(f\"\\nðŸ” FILE BOUNDARY ANALYSIS:\")\n    print(f\"  Time6.txt last record: {time6_max}\")\n    print(f\"  Time.txt first record: {time7_min}\")\n    \n    if time6_max > time7_min:\n        overlap_duration = (time6_max - time7_min).total_seconds() / 60\n        print(f\"  âš ï¸ Time overlap detected: {overlap_duration:.1f} minutes\")\n        \n        # Check for duplicate transactions in overlap\n        overlap_records = df_sorted[\n            (df_sorted['datetime'] >= time7_min) &\n            (df_sorted['datetime'] <= time6_max)\n        ]\n        overlap_from_time6 = overlap_records[overlap_records['source_file'] == 'time6.txt']\n        overlap_from_time7 = overlap_records[overlap_records['source_file'] == 'time.txt']\n        \n        print(f\"    Records from time6.txt in overlap: {len(overlap_from_time6)}\")\n        print(f\"    Records from time.txt in overlap: {len(overlap_from_time7)}\")\n        \n        # Check for exact duplicates\n        duplicates = df_sorted.duplicated(['datetime', 'pid', 'transaction_id'], keep=False)\n        if duplicates.sum() > 0:\n            print(f\"    ðŸ”´ Duplicate transactions found: {duplicates.sum()}\")\n        else:\n            print(f\"    âœ… No duplicate transactions found\")\n    else:\n        gap_duration = (time7_min - time6_max).total_seconds() / 60\n        print(f\"  ðŸ“Š Time gap between files: {gap_duration:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ” DATA QUALITY ASSESSMENT\")\nprint(\"=\" * 40)\n\n# Overall statistics\nvalid_records = df_sorted[df_sorted['is_valid']]\ninvalid_records = df_sorted[~df_sorted['is_valid']]\n\nprint(f\"ðŸ“Š DATASET OVERVIEW:\")\nprint(f\"  Total records: {len(df_sorted):,}\")\nprint(f\"  Valid records: {len(valid_records):,} ({len(valid_records)/len(df_sorted)*100:.1f}%)\")\nprint(f\"  Invalid records: {len(invalid_records):,} ({len(invalid_records)/len(df_sorted)*100:.1f}%)\")\n\n# Time continuity analysis\nprint(f\"\\nâ° TIME CONTINUITY:\")\nif len(valid_records) > 1:\n    time_diffs = valid_records['datetime'].diff().dropna()\n    \n    print(f\"  Average time between records: {time_diffs.mean().total_seconds():.1f} seconds\")\n    print(f\"  Median time between records: {time_diffs.median().total_seconds():.1f} seconds\")\n    print(f\"  Maximum gap: {time_diffs.max().total_seconds() / 60:.1f} minutes\")\n    \n    # Identify significant gaps\n    large_gaps = time_diffs[time_diffs > timedelta(minutes=5)]\n    if len(large_gaps) > 0:\n        print(f\"  ðŸ”´ Large gaps (>5min): {len(large_gaps)}\")\n        print(f\"    Largest gap: {large_gaps.max().total_seconds() / 60:.1f} minutes\")\n    else:\n        print(f\"  âœ… No significant gaps detected\")\n\n# Performance data quality\nif len(valid_records) > 0:\n    exec_times = valid_records['execution_time']\n    \n    print(f\"\\nðŸ“ˆ PERFORMANCE DATA QUALITY:\")\n    print(f\"  Min execution time: {exec_times.min():.3f}s\")\n    print(f\"  Max execution time: {exec_times.max():.1f}s\")\n    print(f\"  Mean execution time: {exec_times.mean():.3f}s\")\n    print(f\"  Median execution time: {exec_times.median():.3f}s\")\n    \n    # Check for anomalies\n    extremely_fast = (exec_times < 0.01).sum()\n    extremely_slow = (exec_times > 120).sum()\n    \n    print(f\"  Extremely fast (<0.01s): {extremely_fast:,}\")\n    print(f\"  Extremely slow (>2min): {extremely_slow:,}\")\n\n# Agent and PID analysis\nif len(valid_records) > 0:\n    print(f\"\\nðŸ”§ SYSTEM COMPONENTS:\")\n    print(f\"  Unique agent types: {valid_records['agent_type'].nunique()}\")\n    print(f\"  Agent type distribution: {valid_records['agent_type'].value_counts().to_dict()}\")\n    print(f\"  Unique PIDs: {valid_records['pid'].nunique()}\")\n    print(f\"  PID range: {valid_records['pid'].min()} - {valid_records['pid'].max()}\")\n\n# Show validation issues summary\nif time6_issues or time7_issues:\n    all_issues = time6_issues + time7_issues\n    print(f\"\\nâš ï¸ VALIDATION ISSUES SUMMARY:\")\n    \n    # Group issues by type\n    issue_types = {}\n    for issue in all_issues:\n        issue_type = issue['issue']\n        if issue_type not in issue_types:\n            issue_types[issue_type] = 0\n        issue_types[issue_type] += 1\n    \n    for issue_type, count in sorted(issue_types.items()):\n        print(f\"    {issue_type}: {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ’¾ EXPORTING CLEAN DATASET\")\nprint(\"=\" * 35)\n\n# Create clean subset (only valid records)\ndf_clean_final = valid_records[[\n    'datetime', 'agent_type', 'pid', 'transaction_id', 'execution_time', \n    'source_file', 'line_number'\n]].copy()\n\n# Add derived fields for analysis\ndf_clean_final['date'] = df_clean_final['datetime'].dt.date\ndf_clean_final['hour'] = df_clean_final['datetime'].dt.hour\ndf_clean_final['minute'] = df_clean_final['datetime'].dt.minute\ndf_clean_final['is_slow'] = df_clean_final['execution_time'] > 20\ndf_clean_final['is_very_slow'] = df_clean_final['execution_time'] > 30\ndf_clean_final['is_critical'] = df_clean_final['execution_time'] > 60\n\n# Export to CSV for further analysis\nclean_filename = 'cleaned_eta_logs.csv'\ndf_clean_final.to_csv(clean_filename, index=False)\n\nprint(f\"âœ… Clean dataset exported: {clean_filename}\")\nprint(f\"  Records: {len(df_clean_final):,}\")\nprint(f\"  Columns: {len(df_clean_final.columns)}\")\nprint(f\"  File size: {os.path.getsize(clean_filename) / 1024 / 1024:.1f} MB\")\n\n# Export validation report\nvalidation_report = {\n    'total_raw_records': len(df_combined),\n    'valid_records': len(valid_records),\n    'invalid_records': len(invalid_records),\n    'time_range': {\n        'start': str(df_clean_final['datetime'].min()),\n        'end': str(df_clean_final['datetime'].max()),\n        'duration_hours': (df_clean_final['datetime'].max() - df_clean_final['datetime'].min()).total_seconds() / 3600\n    },\n    'data_sources': {\n        'time6_records': len(df_time6_clean),\n        'time7_records': len(df_time7_clean)\n    }\n}\n\nprint(f\"\\nðŸ“‹ FINAL DATASET SUMMARY:\")\nfor key, value in validation_report.items():\n    if isinstance(value, dict):\n        print(f\"  {key}:\")\n        for subkey, subvalue in value.items():\n            print(f\"    {subkey}: {subvalue}\")\n    else:\n        print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")\n\nprint(f\"\\nðŸŽ‰ DATA CLEANING COMPLETE!\")\nprint(f\"   Clean dataset ready for incident analysis\")\nprint(f\"   Use '{clean_filename}' for all subsequent analysis\")\nprint(f\"   Data quality: {len(valid_records)/len(df_combined)*100:.1f}% valid records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Incident Timeline Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification of incident timeframe with clean data\nprint(\"\\nðŸŽ¯ INCIDENT TIMELINE VERIFICATION WITH CLEAN DATA\")\nprint(\"=\" * 55)\n\nincident_time = datetime(2025, 11, 6, 22, 10)\nanalysis_start = datetime(2025, 11, 6, 21, 30)\nanalysis_end = datetime(2025, 11, 7, 3, 30)\n\n# Filter to analysis window\nanalysis_data = df_clean_final[\n    (df_clean_final['datetime'] >= analysis_start) &\n    (df_clean_final['datetime'] <= analysis_end)\n].copy()\n\nprint(f\"ðŸ“Š ANALYSIS WINDOW DATA:\")\nprint(f\"  Records in analysis window: {len(analysis_data):,}\")\nprint(f\"  Time range: {analysis_data['datetime'].min()} â†’ {analysis_data['datetime'].max()}\")\n\nif len(analysis_data) > 0:\n    # Hour-by-hour breakdown\n    hourly = analysis_data.groupby(analysis_data['datetime'].dt.floor('H')).agg({\n        'execution_time': ['count', 'mean'],\n        'is_slow': 'sum',\n        'is_critical': 'sum'\n    }).round(2)\n    \n    hourly.columns = ['count', 'avg_time', 'slow_count', 'critical_count']\n    hourly['slow_pct'] = (hourly['slow_count'] / hourly['count'] * 100).round(1)\n    \n    print(f\"\\nâ° HOURLY BREAKDOWN (clean data):\")\n    for hour, row in hourly.iterrows():\n        print(f\"  {hour.strftime('%H:00')}: {row['count']:>4} txns, {row['avg_time']:>5.1f}s avg, {row['slow_pct']:>4.1f}% slow\")\n    \n    # Identify peak issues\n    worst_hour = hourly['avg_time'].idxmax()\n    highest_slow_rate = hourly['slow_pct'].idxmax()\n    \n    print(f\"\\nðŸš¨ CLEAN DATA INSIGHTS:\")\n    print(f\"  Worst performance: {worst_hour.strftime('%H:00')} ({hourly.loc[worst_hour, 'avg_time']:.1f}s avg)\")\n    print(f\"  Highest slow rate: {highest_slow_rate.strftime('%H:00')} ({hourly.loc[highest_slow_rate, 'slow_pct']:.1f}%)\")\n    \n    # Critical transactions\n    critical_txns = analysis_data[analysis_data['is_critical']]\n    if len(critical_txns) > 0:\n        print(f\"  Critical transactions (>60s): {len(critical_txns):,}\")\n        worst_txn = critical_txns.loc[critical_txns['execution_time'].idxmax()]\n        print(f\"  Worst transaction: {worst_txn['execution_time']:.1f}s at {worst_txn['datetime'].strftime('%H:%M:%S')}\")\n\nprint(f\"\\nâœ… Clean data ready for accurate incident analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}